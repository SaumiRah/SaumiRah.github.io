[
  {
    "objectID": "projects/RAship-Data-Analysis.html",
    "href": "projects/RAship-Data-Analysis.html",
    "title": "RAship Code",
    "section": "",
    "text": "The following is an R project I developed to perform data analysis on historic economic data for my position as a Research Assistant for Prof. Nicolas Schmitt at Simon Fraser University. The version of this file published on Github will load redacted versions of proprietary data. All categorical variables are aliased, and all numeric fields are shuffled at random to preserve confidentiality. File names and headers are kept the same.\nNote from the future: If I were to tackle this project today I would approach the task much differently: I would employ a more conrete naming convention for variables and files, use version control, and would overall use more functions for DRYer code. I include this project here because it served as an important learning opportunity for me, and demonstrates that I’ve solved a real-world assignment."
  },
  {
    "objectID": "projects/RAship-Data-Analysis.html#acknowledgements",
    "href": "projects/RAship-Data-Analysis.html#acknowledgements",
    "title": "RAship Code",
    "section": "",
    "text": "The following is an R project I developed to perform data analysis on historic economic data for my position as a Research Assistant for Prof. Nicolas Schmitt at Simon Fraser University. The version of this file published on Github will load redacted versions of proprietary data. All categorical variables are aliased, and all numeric fields are shuffled at random to preserve confidentiality. File names and headers are kept the same.\nNote from the future: If I were to tackle this project today I would approach the task much differently: I would employ a more conrete naming convention for variables and files, use version control, and would overall use more functions for DRYer code. I include this project here because it served as an important learning opportunity for me, and demonstrates that I’ve solved a real-world assignment."
  },
  {
    "objectID": "projects/RAship-Data-Analysis.html#summarizing-function-recategorized-data",
    "href": "projects/RAship-Data-Analysis.html#summarizing-function-recategorized-data",
    "title": "RAship Code",
    "section": "Summarizing Function: Recategorized Data",
    "text": "Summarizing Function: Recategorized Data\nnameG &lt;- paste(\"Cat\", seq(1,17), sep = \".\")\n\nrecatData &lt;- function(file_path) { #defining summary function\n\n  data &lt;- read_excel(file_path)\n  data$Category &lt;- as.factor(data$Category)\n  data$`General Import Tax` &lt;- sample(data$`General Import Tax`) #randomizing values\n  \n  data_summary &lt;- data %&gt;%\n    group_by(Category) %&gt;%\n    summarise_at(vars(`General Import Tax`), list(mean=mean, sd=sd)) %&gt;%\n    as.data.frame()\n\n  data_count &lt;- data %&gt;% dplyr::count(Category)\n  data_merged &lt;- left_join(data_summary, data_count, by= 'Category')\n  levels(data_merged$Category) &lt;- nameG\n  return(data_merged[-c(17), ])\n\n}\n\n1902\noTwoDF &lt;- recatData(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1902_Recat_V3.xlsx\")\nhead(oTwoDF) # example of year = 1902\n##   Category     mean        sd   n\n## 1    Cat.1 32.24651  56.68118 129\n## 2    Cat.2 51.88235 101.51224  17\n## 3    Cat.3 27.00161  32.95000  31\n## 4    Cat.4 67.33871 106.33315  31\n## 5    Cat.5 24.38889  39.31782  18\n## 6    Cat.6 39.35234  79.68471  64"
  },
  {
    "objectID": "projects/RAship-Data-Analysis.html#section-1",
    "href": "projects/RAship-Data-Analysis.html#section-1",
    "title": "RAship Code",
    "section": "1891",
    "text": "1891\nnineOneDF &lt;- recatData(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1891_Recat_V2.xlsx\")\nhead(nineOneDF) # example of year = 1891\n##   Category     mean       sd  n\n## 1    Cat.1 31.44384 55.13447 73\n## 2    Cat.2 29.25000 39.12935 14\n## 3    Cat.3 20.35000 33.41192 24\n## 4    Cat.4 13.50769 15.91035 13\n## 5    Cat.5 15.30714 24.42592  7\n## 6    Cat.6 22.79333 32.87008 30"
  },
  {
    "objectID": "projects/RAship-Data-Analysis.html#section-2",
    "href": "projects/RAship-Data-Analysis.html#section-2",
    "title": "RAship Code",
    "section": "1887",
    "text": "1887\neightSevDF &lt;-recatData(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1891_Recat_V2.xlsx\")\nhead(eightSevDF)\n##   Category     mean       sd  n\n## 1    Cat.1 32.47260 65.17196 73\n## 2    Cat.2 30.52143 39.16315 14\n## 3    Cat.3 25.12500 41.59255 24\n## 4    Cat.4 27.08462 41.09249 13\n## 5    Cat.5 34.92857 65.38376  7\n## 6    Cat.6 30.63500 39.58481 30"
  },
  {
    "objectID": "projects/RAship-Data-Analysis.html#section-3",
    "href": "projects/RAship-Data-Analysis.html#section-3",
    "title": "RAship Code",
    "section": "1884",
    "text": "1884\neightFourDF &lt;- recatData(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1884_Recat_V2.xlsx\")\nhead(eightFourDF)\n##   Category     mean       sd  n\n## 1    Cat.1 15.73359 21.27716 78\n## 2    Cat.2 10.60417 10.90530 12\n## 3    Cat.3 16.12500 22.99966 26\n## 4    Cat.4 17.49167 18.87497 12\n## 5    Cat.5 15.11111 14.17475  9\n## 6    Cat.6 17.26167 24.61115 30"
  },
  {
    "objectID": "projects/RAship-Data-Analysis.html#accepted",
    "href": "projects/RAship-Data-Analysis.html#accepted",
    "title": "RAship Code",
    "section": "1851_accepted",
    "text": "1851_accepted\nfiveOneDF &lt;- recatData(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1851_accepted_Recat_V3.xlsx\")\nhead(fiveOneDF)\n##   Category      mean        sd  n\n## 1    Cat.1 4.3239130 4.6650800 46\n## 2    Cat.2 4.2115385 4.4371610 13\n## 3    Cat.3 0.9384615 0.8677446 13\n## 4    Cat.4 6.7083333 5.4369492  6\n## 5    Cat.5 4.7100000 6.1842946  5\n## 6    Cat.6 5.2285714 5.8470477 14"
  },
  {
    "objectID": "projects/RAship-Data-Analysis.html#section-4",
    "href": "projects/RAship-Data-Analysis.html#section-4",
    "title": "RAship Code",
    "section": "1849",
    "text": "1849\nfourNine &lt;- read_excel(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1849_Recat_V4.xlsx\")\nfourNine$Category&lt;-as.factor(fourNine$Category)\n\n#converting Batz to Franc\nfourNine &lt;- fourNine %&gt;% \n  mutate(`General Import Tax` = Batz / 7)\n\nfourNine$`General Import Tax` &lt;- sample(fourNine$`General Import Tax`) #randomizing values\n\nfourNineT &lt;- fourNine %&gt;%\n  group_by(Category) %&gt;%\n  summarise_at(vars(`General Import Tax`), list(mean=mean, sd=sd)) %&gt;% \n  as.data.frame()\n\nfourNineN &lt;- fourNine %&gt;% dplyr::count(Category)\nfourNineDF &lt;- left_join(fourNineT, fourNineN, by= join_by(Category))\n\n\nlevels(fourNineDF$Category) &lt;- nameG\n\nfourNineDF &lt;- fourNineDF[-c(17), ]"
  },
  {
    "objectID": "projects/RAship-Data-Analysis.html#creating-nominal-master-data-frame-1902-1891-1887-1884-1851-and-1849",
    "href": "projects/RAship-Data-Analysis.html#creating-nominal-master-data-frame-1902-1891-1887-1884-1851-and-1849",
    "title": "RAship Code",
    "section": "Creating Nominal Master Data Frame: 1902, 1891, 1887, 1884, 1851, and 1849",
    "text": "Creating Nominal Master Data Frame: 1902, 1891, 1887, 1884, 1851, and 1849\n## Master DF... needs to be pivoted\n{ppp &lt;- left_join(oTwoDF, nineOneDF, by = join_by(Category));\nTppp &lt;- left_join(ppp, eightSevDF, by = join_by(Category));\nCppp &lt;- left_join(Tppp, eightFourDF, by = join_by(Category));\nGppp &lt;- left_join(Cppp, fiveOneDF, by = join_by(Category));\nQppp&lt;- left_join(Gppp, fourNineDF, by = join_by(Category));\n}\n\n## Pivoting Master DF long...\n# Pivoting Mean\n{mmm = subset(Qppp, select = c(Category, mean.x, mean.y, mean.x.x, mean.y.y, mean.x.x.x, mean.y.y.y));\nmmm &lt;- mmm %&gt;%\n  pivot_longer(cols=-Category, names_to = 'Year', names_prefix = 'mean.', values_to = \"mean\");\n  \nmmm$Year[mmm$Year == \"x\"] = \"1902\"; mmm$Year[mmm$Year == \"y\"] = \"1891\";\n  mmm$Year[mmm$Year == \"x.x\"] = \"1887\"; mmm$Year[mmm$Year == \"y.y\"] = \"1884\"; mmm$Year[mmm$Year == \"x.x.x\"]=\"1851\";\n  mmm$Year[mmm$Year == \"y.y.y\"]=\"1849\"\n}\n\n\n# Pivoting SD\n{sdDF = subset(Qppp, select = c(Category, sd.x, sd.y, sd.x.x, sd.y.y, sd.x.x.x, sd.y.y.y));\nsdDF &lt;- sdDF %&gt;%\n  pivot_longer(cols=-Category, names_to = 'Year', names_prefix = 'sd.', values_to = \"sd\");\n  \nsdDF$Year[sdDF$Year == \"x\"] = \"1902\"; sdDF$Year[sdDF$Year == \"y\"] = \"1891\"; sdDF$Year[sdDF$Year == \"sd\"] = \"1887\" ;\n  sdDF$Year[sdDF$Year == \"x.x\"] = \"1887\"; sdDF$Year[sdDF$Year == \"y.y\"] = \"1884\"; sdDF$Year[sdDF$Year == \"x.x.x\"] = \"1851\";\n  sdDF$Year[sdDF$Year == \"y.y.y\"]=\"1849\"\n}\n\n# Pivoting n\n{nDF = subset(Qppp, select = c(Category, n.x, n.y, n.x.x, n.y.y, n.x.x.x, n.y.y.y));\nnDF &lt;- nDF %&gt;%\n  pivot_longer(cols=-Category, names_to = 'Year', names_prefix = 'n.', values_to = \"n\")\n  \nnDF$Year[nDF$Year == \"x\"] = \"1902\"; nDF$Year[nDF$Year == \"y\"] = \"1891\"; nDF$Year[nDF$Year == \"n\"] = \"1887\";\n  nDF$Year[nDF$Year == \"x.x\"] = \"1887\"; nDF$Year[nDF$Year == \"y.y\"] = \"1884\"; nDF$Year[nDF$Year == \"x.x.x\"] = \"1851\";\n  nDF$Year[nDF$Year == \"y.y.y\"] = \"1849\"\n}\n\n\n# joining long-pivoted DFs into final DF\nfDFt &lt;- left_join(mmm, sdDF, by= join_by(Category, Year))\nfDF &lt;- left_join(fDFt, nDF, by= join_by(Category, Year))\nfDF[is.na(fDF)] &lt;- 0 # Final Dataframe: \"fDF\"\n# Optional code to write to xlsx file\n#Write to excel file (warning: will overwrite file with same name. Make sure to change name and path) \n#write_xlsx(fDF, \"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1902-1849 Master - Real_V2.xlsx\")"
  },
  {
    "objectID": "projects/RAship-Data-Analysis.html#real-values",
    "href": "projects/RAship-Data-Analysis.html#real-values",
    "title": "RAship Code",
    "section": "Real Values",
    "text": "Real Values\nAccounting for a price index; Without accounting for index = “Nominal values”\nnameG &lt;- paste(\"Cat\", seq(1,17), sep = \".\")\n\nrealData &lt;- function(df) { #defining summary function\n\n  data &lt;- df\n  data$Category &lt;- as.factor(data$Category)\n  data$`General Import Tax` &lt;- sample(data$`General Import Tax`) #randomizing values\n  \n  data_summary &lt;- data %&gt;%\n    group_by(Category) %&gt;%\n    summarise_at(vars(`General Import Tax`), list(mean=mean, sd=sd)) %&gt;%\n    as.data.frame()\n\n  data_count &lt;- data %&gt;% dplyr::count(Category)\n  data_merged &lt;- left_join(data_summary, data_count, by= 'Category')\n  levels(data_merged$Category) &lt;- nameG\n  return(data_merged[-c(17), ])\n\n}\n\n\nnineOneR &lt;- read_xlsx(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1891_Recat_V2.xlsx\") %&gt;% \n    mutate(`General Import Tax` = `General Import Tax` / 1.048)\n\neightSevR &lt;- read_xlsx(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1887_Recat_V2.xlsx\") %&gt;% \n    mutate(`General Import Tax` = `General Import Tax` / .922)\n\neightFourR &lt;- read_xlsx(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1884_Recat_V2.xlsx\") %&gt;% \n    mutate(`General Import Tax` = `General Import Tax` / 1.051)\n\nfiveOneR &lt;- read_xlsx(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1851_accepted_Recat_V3.xlsx\") %&gt;% \n    mutate(`General Import Tax` = `General Import Tax` / 0.864)\n\nfourNineR &lt;- read_xlsx(\"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1849_Recat_V4.xlsx\") %&gt;% \n  mutate(`General Import Tax` = Batz / 7)%&gt;%\n  mutate(`General Import Tax` = `General Import Tax` / 0.745)\n\nRnineOneDF &lt;- realData(nineOneR)\nReightSevDF &lt;- realData(eightSevR)\nReightFourDF &lt;- realData(eightFourR)\nRfiveOneDF &lt;- realData(fiveOneR)\nRfourNineDF &lt;- realData(fourNineR)"
  },
  {
    "objectID": "projects/RAship-Data-Analysis.html#creating-real-master-data-frame-1902-1891-1887-1884-1851-and-1849",
    "href": "projects/RAship-Data-Analysis.html#creating-real-master-data-frame-1902-1891-1887-1884-1851-and-1849",
    "title": "RAship Code",
    "section": "Creating Real Master Data Frame: 1902, 1891, 1887, 1884, 1851, and 1849",
    "text": "Creating Real Master Data Frame: 1902, 1891, 1887, 1884, 1851, and 1849\n## Master DF... needs to be pivoted\n{Rppp &lt;- left_join(oTwoDF, RnineOneDF, by = join_by(Category));\nTRppp &lt;- left_join(Rppp, ReightSevDF, by = join_by(Category));\nCRppp &lt;- left_join(TRppp, ReightFourDF, by = join_by(Category));\nGRppp &lt;- left_join(CRppp, RfiveOneDF, by = join_by(Category));\nQRppp&lt;- left_join(GRppp, RfourNineDF, by = join_by(Category));\n}\n\n## Pivoting Master DF long...\n# Pivoting Mean\n{Rmmm = subset(QRppp, select = c(Category, mean.x, mean.y, mean.x.x, mean.y.y, mean.x.x.x, mean.y.y.y));\nRmmm &lt;- Rmmm %&gt;%\n  pivot_longer(cols=-Category, names_to = 'Year', names_prefix = 'mean.', values_to = \"mean\");\n  \nRmmm$Year[Rmmm$Year == \"x\"] = \"1902\"; Rmmm$Year[Rmmm$Year == \"y\"] = \"1891\";\n  Rmmm$Year[Rmmm$Year == \"x.x\"] = \"1887\"; Rmmm$Year[Rmmm$Year == \"y.y\"] = \"1884\"; Rmmm$Year[Rmmm$Year == \"x.x.x\"]=\"1851\";\n  Rmmm$Year[Rmmm$Year == \"y.y.y\"]=\"1849\"\n}\n\n\n# Pivoting SD\n{RsdDF = subset(QRppp, select = c(Category, sd.x, sd.y, sd.x.x, sd.y.y, sd.x.x.x, sd.y.y.y));\nRsdDF &lt;- RsdDF %&gt;%\n  pivot_longer(cols=-Category, names_to = 'Year', names_prefix = 'sd.', values_to = \"sd\");\n  \nRsdDF$Year[RsdDF$Year == \"x\"] = \"1902\"; RsdDF$Year[RsdDF$Year == \"y\"] = \"1891\"; RsdDF$Year[RsdDF$Year == \"sd\"] = \"1887\" ;\n  RsdDF$Year[RsdDF$Year == \"x.x\"] = \"1887\"; RsdDF$Year[RsdDF$Year == \"y.y\"] = \"1884\"; RsdDF$Year[RsdDF$Year == \"x.x.x\"] = \"1851\";\n  RsdDF$Year[RsdDF$Year == \"y.y.y\"]=\"1849\"\n}\n\n# Pivoting n\n{RnDF = subset(QRppp, select = c(Category, n.x, n.y, n.x.x, n.y.y, n.x.x.x, n.y.y.y));\nRnDF &lt;- RnDF %&gt;%\n  pivot_longer(cols=-Category, names_to = 'Year', names_prefix = 'n.', values_to = \"n\")\n  \nRnDF$Year[RnDF$Year == \"x\"] = \"1902\"; RnDF$Year[RnDF$Year == \"y\"] = \"1891\"; RnDF$Year[RnDF$Year == \"n\"] = \"1887\";\n  RnDF$Year[RnDF$Year == \"x.x\"] = \"1887\"; RnDF$Year[RnDF$Year == \"y.y\"] = \"1884\"; RnDF$Year[RnDF$Year == \"x.x.x\"] = \"1851\";\n  RnDF$Year[RnDF$Year == \"y.y.y\"] = \"1849\"\n}\n\n\n# joining long-pivoted DFs into final DF\nRfDFt &lt;- left_join(Rmmm, RsdDF, by= join_by(Category, Year))\nRealfDF &lt;- left_join(RfDFt, RnDF, by= join_by(Category, Year))\nRealfDF[is.na(RealfDF)] &lt;- 0 # Final Dataframe: \"fDF\"\n# Optional code to write to xlsx file\n#Write to excel file (warning: will overwrite file with same name. Make sure to change name and path) \n#write_xlsx(fDF, \"D:\\\\Everything\\\\School\\\\RA position\\\\Tariff Schedules\\\\Analysis\\\\Sector Recategorization\\\\1902-1849 Master - Real_V2.xlsx\")"
  },
  {
    "objectID": "projects/Optimizing_Gratuitiy.html#section-iii-part-1.-bivariate-analysis",
    "href": "projects/Optimizing_Gratuitiy.html#section-iii-part-1.-bivariate-analysis",
    "title": "Optimizing Gratuity",
    "section": "Section III, Part 1. Bivariate analysis",
    "text": "Section III, Part 1. Bivariate analysis\ntpdlm &lt;- lm(TipSum ~ `Day of Week`, data = tpp)  # Defining the linear model representing the bivariate relationship between TipSum and the Day of the Week for hypothesis 1\nsummary(tpdlm)\n## \n## Call:\n## lm(formula = TipSum ~ `Day of Week`, data = tpp)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -41.32 -17.59  -6.99   9.82  94.71 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)       40.2808     8.0140   5.026  4.4e-06 ***\n## `Day of Week`Mon -14.2190    11.5882  -1.227    0.224    \n## `Day of Week`Tue -15.8708    11.8867  -1.335    0.187    \n## `Day of Week`Wed  -0.3965    13.2031  -0.030    0.976    \n## `Day of Week`Thu -17.8696    12.6713  -1.410    0.163    \n## `Day of Week`Sat   9.8692    11.5882   0.852    0.398    \n## `Day of Week`Sun  -5.3836    11.5882  -0.465    0.644    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27.76 on 63 degrees of freedom\n## Multiple R-squared:  0.1144, Adjusted R-squared:  0.03001 \n## F-statistic: 1.356 on 6 and 63 DF,  p-value: 0.2464\ntpwlm &lt;- lm(TipSum ~ Weekend, data = tpp)  # Defining the linear model representing the bivariate relationship between TipSum and the Weekend dummy variable for hypothesis 2\nsummary(tpwlm)\n## \n## Call:\n## lm(formula = TipSum ~ Weekend, data = tpp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -39.882 -18.343  -6.676   7.905 103.128 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)      27.479      4.576   6.005 8.35e-08 ***\n## WeekendWeekend   14.253      6.567   2.170   0.0335 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27.46 on 68 degrees of freedom\n## Multiple R-squared:  0.06479,    Adjusted R-squared:  0.05104 \n## F-statistic: 4.711 on 1 and 68 DF,  p-value: 0.03347\nRegarding my first hypothesis, the bivariate regression between the calendar day and cash-outs of all given dates does not produce any significant results; we cannot say with certainty that any particular day of the week is correlated with higher tip-outs. I could not even begin to reject the main null hypothesis based on these results.\nTurning to my second hypothesis, the bivariate regression between weekday/weekend status and cash-outs does produce significant results. We see that on average, weekends are correlated with a $14.25 increase in tips compared to weekdays: on average I make around $27.48 in tips on weekdays and $41.73 on weekends.\nThough accounting for our control variables for our first hypothesis would theoretically produce a more accurate model, given the sheer insignificance of these results a more precise model in the form of a multiple regression analysis isn’t likely to produce results sufficient to reject the null. Unlike the bivariate regression of our first model, the model for our second hypothesis could very well benefit from the addition of precipitation and order amount as control variables. It’s completely possible that accounting for these two un-modeled variables could rob significance from our results above.\nlibrary(ggplot2)\nlibrary(ggthemes)\n# visual representation of average tips for each research category and their respective hypotheses\nggplot(data=tpp, aes(x=`Day of Week`, y=TipSum)) +\n  geom_bar(stat=\"summary\", aes(fill=`Day of Week`), size=2)+\n  labs(x=\"\", y=\"Tips ($)\")+\n  theme_economist()+\n  geom_smooth(method = lm)+\n  theme(legend.position = \"none\")+\n  ggtitle(\"Average Tips by Day of Week\")\n\nggplot(data=tpp, aes(x=Weekend, y=TipSum)) +\n  geom_bar(stat=\"summary\", aes(fill=Weekend), size=1)+\n  labs(x=\"\", y=\"Tips ($)\")+\n  theme_economist()+\n  geom_smooth(method = lm)+\n  theme(legend.position = \"none\",\n          text = element_text(size=10))+\n  ggtitle(\"Average Tips by Weekday/Weekend\")"
  },
  {
    "objectID": "projects/Optimizing_Gratuitiy.html#section-iii-part-2.-multiple-regression",
    "href": "projects/Optimizing_Gratuitiy.html#section-iii-part-2.-multiple-regression",
    "title": "Optimizing Gratuity",
    "section": "Section III, Part 2. Multiple Regression",
    "text": "Section III, Part 2. Multiple Regression\ntpdlmm &lt;- lm(TipSum ~ `Day of Week` + AmountSum + Precipitation, data= tpp) # Defining the multivariate model representing multiple regressions analysis\nsummary(tpdlmm)\n## \n## Call:\n## lm(formula = TipSum ~ `Day of Week` + AmountSum + Precipitation, \n##     data = tpp)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -16.6835  -2.8527   0.8836   3.6846  11.7521 \n## \n## Coefficients:\n##                         Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)             -1.70692    2.14829  -0.795   0.4301    \n## `Day of Week`Mon         2.69317    2.41052   1.117   0.2684    \n## `Day of Week`Tue         1.82903    2.49356   0.734   0.4662    \n## `Day of Week`Wed         4.42647    2.70523   1.636   0.1071    \n## `Day of Week`Thu        -0.16291    2.66630  -0.061   0.9515    \n## `Day of Week`Sat         3.76959    2.39709   1.573   0.1212    \n## `Day of Week`Sun         5.83434    2.44290   2.388   0.0201 *  \n## AmountSum                0.13590    0.00364  37.340   &lt;2e-16 ***\n## PrecipitationLight Rain -0.34361    2.01601  -0.170   0.8652    \n## PrecipitationNo Rain     0.47589    1.91859   0.248   0.8050    \n## PrecipitationHeavy Rain -1.41900    1.86852  -0.759   0.4506    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.663 on 59 degrees of freedom\n## Multiple R-squared:  0.9655, Adjusted R-squared:  0.9596 \n## F-statistic: 165.1 on 10 and 59 DF,  p-value: &lt; 2.2e-16\ntpwlmm &lt;- lm(TipSum ~ Weekend + AmountSum + Precipitation, data=tpp) # Doing the same for the second hypothesis\nsummary(tpwlmm)\n## \n## Call:\n## lm(formula = TipSum ~ Weekend + AmountSum + Precipitation, data = tpp)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -19.7999  -2.3041   0.7308   2.9308  14.1873 \n## \n## Coefficients:\n##                          Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)              0.160096   1.480976   0.108    0.914    \n## WeekendWeekend           0.890127   1.457937   0.611    0.544    \n## AmountSum                0.135727   0.003616  37.539   &lt;2e-16 ***\n## PrecipitationLight Rain  0.101026   2.037916   0.050    0.961    \n## PrecipitationNo Rain     0.954759   1.894097   0.504    0.616    \n## PrecipitationHeavy Rain -0.718763   1.863535  -0.386    0.701    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.826 on 64 degrees of freedom\n## Multiple R-squared:  0.9604, Adjusted R-squared:  0.9573 \n## F-statistic: 310.2 on 5 and 64 DF,  p-value: &lt; 2.2e-16\nWhen controlling for the order amount and the rain level we can see that the order amount is significantly correlated with higher tips for both hypotheses. In both cases, we see that a one dollar increase in the order amount is correlated with a roughly $0.14 increase in tips. This makes a lot of sense, because tipping 15% of the order amount is considered the norm, and a $0.14 increase in tips for every dollar increase in amount is roughly a 15% increase. Furthermore, we saw that precipitation was not significantly correlated with cash-outs.\nRegarding our first hypothesis, we now see that Sundays are significantly correlated with a $5.83 increase in tips compared to Friday. Though this result is significant, it is not enough information to confirm our hypothesis, because we don’t have significant values for Saturday or Friday to compare Sunday to. This confirms my suspicion that multiple regression does not make any progress in confirming my main hypothesis.\nRegarding our second hypothesis, weekends are no longer significantly correlated with tips. This undermines our capacity to confirm our second hypothesis, and reject our second null."
  },
  {
    "objectID": "projects/Optimizing_Gratuitiy.html#section-iv-part-1.-linearity",
    "href": "projects/Optimizing_Gratuitiy.html#section-iv-part-1.-linearity",
    "title": "Optimizing Gratuity",
    "section": "Section IV, Part 1. Linearity",
    "text": "Section IV, Part 1. Linearity\nlibrary(car)\ncrPlot(tpdlmm, \"AmountSum\")\n\ncrPlot(tpwlmm, \"AmountSum\")\n\nNon-linearity is when the relationship between an independent variable and a dependent variable is not linear, i.e. not a straight line; a curved line. Un-modelled non-linearity is a problem because it means a linear model does not accurately represent the correlation between the outcome and research variable. To test our assumption of linearity, we’ll plot the residuals (the difference between the predicted value of tips and the actual values) of our Tip variable against the order amount. We’re looking for large deviations of residual (traced by a smooth pink line) from the expected values (the dashed blue line), which we don’t see for either hypothesis. This suggests that it’s safe to assume our model is linear."
  },
  {
    "objectID": "projects/Optimizing_Gratuitiy.html#section-iv-part-2.-additivity",
    "href": "projects/Optimizing_Gratuitiy.html#section-iv-part-2.-additivity",
    "title": "Optimizing Gratuity",
    "section": "Section IV, Part 2. Additivity",
    "text": "Section IV, Part 2. Additivity\ndwamint &lt;- lm(TipSum ~ `Day of Week` + AmountSum + Precipitation + `Day of Week`*AmountSum, data=tpp)\nsummary(dwamint)\n## \n## Call:\n## lm(formula = TipSum ~ `Day of Week` + AmountSum + Precipitation + \n##     `Day of Week` * AmountSum, data = tpp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.174  -2.754   1.121   2.858  11.131 \n## \n## Coefficients:\n##                             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                -3.121128   2.827903  -1.104   0.2747    \n## `Day of Week`Mon            1.701746   4.370748   0.389   0.6986    \n## `Day of Week`Tue            2.952111   4.512479   0.654   0.5158    \n## `Day of Week`Wed            0.233517   4.386120   0.053   0.9577    \n## `Day of Week`Thu            1.445962   4.177960   0.346   0.7306    \n## `Day of Week`Sat            9.135624   3.896596   2.345   0.0228 *  \n## `Day of Week`Sun            4.440349   3.997083   1.111   0.2716    \n## AmountSum                   0.138684   0.006773  20.475   &lt;2e-16 ***\n## PrecipitationLight Rain     1.211983   2.119148   0.572   0.5698    \n## PrecipitationNo Rain        0.456024   1.988259   0.229   0.8195    \n## PrecipitationHeavy Rain    -0.430723   1.904349  -0.226   0.8219    \n## `Day of Week`Mon:AmountSum  0.007140   0.017823   0.401   0.6903    \n## `Day of Week`Tue:AmountSum -0.002553   0.018792  -0.136   0.8924    \n## `Day of Week`Wed:AmountSum  0.015697   0.012188   1.288   0.2034    \n## `Day of Week`Thu:AmountSum -0.006268   0.015640  -0.401   0.6902    \n## `Day of Week`Sat:AmountSum -0.015622   0.009431  -1.657   0.1035    \n## `Day of Week`Sun:AmountSum  0.006498   0.012400   0.524   0.6025    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.555 on 53 degrees of freedom\n## Multiple R-squared:  0.9702, Adjusted R-squared:  0.9612 \n## F-statistic: 107.7 on 16 and 53 DF,  p-value: &lt; 2.2e-16\nweamint &lt;- lm(TipSum ~ Weekend + AmountSum + Precipitation + Weekend*AmountSum, data=tpp)\nsummary(weamint)\n## \n## Call:\n## lm(formula = TipSum ~ Weekend + AmountSum + Precipitation + Weekend * \n##     AmountSum, data = tpp)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -19.3429  -2.9072   0.5624   2.9719  10.8939 \n## \n## Coefficients:\n##                           Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)              -2.042655   1.915015  -1.067   0.2902    \n## WeekendWeekend            4.130397   2.323774   1.777   0.0803 .  \n## AmountSum                 0.146398   0.006993  20.935   &lt;2e-16 ***\n## PrecipitationLight Rain   0.466863   2.015268   0.232   0.8176    \n## PrecipitationNo Rain      0.512134   1.879861   0.272   0.7862    \n## PrecipitationHeavy Rain  -0.233512   1.853469  -0.126   0.9001    \n## WeekendWeekend:AmountSum -0.014505   0.008185  -1.772   0.0812 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.731 on 63 degrees of freedom\n## Multiple R-squared:  0.9623, Adjusted R-squared:  0.9587 \n## F-statistic: 267.7 on 6 and 63 DF,  p-value: &lt; 2.2e-16\nNon-additivity, also known as an interaction is when the relationship between an independent variable and the outcome variable depends on the values of another variable. Failure to account for interactions makes less accurate linear models. To test additivity, we’ll model an interaction between the day of the week/weekend status and the order amount variable. The reason we’re testing an interaction between these two independent variables, and not precipitation, is because I know from experience that weekends are busier than week days, and therefore have higher values of total order amount. It’s not necessarily that people are ordering more food on weekends, rather there are just more people ordering on weekends. However, even though order amounts are higher on weekends, I believe that people will not be tipping more than their order amount would predict on weekends than weekdays. I hypothesize that the interaction between the day of the week/weekend status will have a negligible correlation with tips, let’s say a positive coefficient that’s less than 1, given that the interaction is in fact significant. To be clear, my additivity hypothesis is a conditional proposition: IF the interaction is significant, THEN the coefficient is positive but less than one.\nSince I already have two working hypotheses, I’ll have to test my additivty hypothesis for both models. Interpreting the multiple regression models including the interaction variable, we see that the relationship between total order amount and cash-out amount is not conditional on which day of the week it happens to be, or if it happens to be a weekend. My conditional hypothesis was vacuously true for both models because the interaction was not significant. (A conditional proposition is true when it’s antecedent clause is false.) I could find no significant interactions in my models, so it’s safe to assume our data is additive."
  },
  {
    "objectID": "projects/Optimizing_Gratuitiy.html#section-iv-part-3.-unusual-cases-heteroskedasticity",
    "href": "projects/Optimizing_Gratuitiy.html#section-iv-part-3.-unusual-cases-heteroskedasticity",
    "title": "Optimizing Gratuity",
    "section": "Section IV, Part 3. Unusual cases & heteroskedasticity",
    "text": "Section IV, Part 3. Unusual cases & heteroskedasticity\n#Normality\n\nggplot(tpp, aes(x=TipSum, y = ..count..))+\n  geom_histogram(aes(fill = ..count..))+\n  scale_y_continuous(name = \"Count\") +\n  ggtitle(\"Frequency of Cash-outs amounts\")+\n  theme(legend.position = \"none\", panel.border = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),)+\n  geom_density(color=\"red\")\n\nlibrary(car)\n\nlogdlmm &lt;- lm(TipSumLog ~ `Day of Week` + AmountSum + Precipitation, data=tpp)\nsummary(logdlmm)\n## \n## Call:\n## lm(formula = TipSumLog ~ `Day of Week` + AmountSum + Precipitation, \n##     data = tpp)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.89039 -0.14611  0.09825  0.28043  0.58108 \n## \n## Coefficients:\n##                           Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)              2.3603002  0.1759364  13.416   &lt;2e-16 ***\n## `Day of Week`Mon         0.1169072  0.1974117   0.592    0.556    \n## `Day of Week`Tue        -0.0132046  0.2042128  -0.065    0.949    \n## `Day of Week`Wed         0.0163281  0.2215474   0.074    0.941    \n## `Day of Week`Thu        -0.2435917  0.2183594  -1.116    0.269    \n## `Day of Week`Sat         0.1393906  0.1963120   0.710    0.480    \n## `Day of Week`Sun         0.0600420  0.2000636   0.300    0.765    \n## AmountSum                0.0034462  0.0002981  11.562   &lt;2e-16 ***\n## PrecipitationLight Rain  0.0492576  0.1651029   0.298    0.766    \n## PrecipitationNo Rain     0.0061726  0.1571246   0.039    0.969    \n## PrecipitationHeavy Rain -0.1019849  0.1530241  -0.666    0.508    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4637 on 59 degrees of freedom\n## Multiple R-squared:  0.7426, Adjusted R-squared:  0.699 \n## F-statistic: 17.02 on 10 and 59 DF,  p-value: 5.693e-14\nlogwlmm &lt;- lm(TipSumLog ~ Weekend + AmountSum + Precipitation, data=tpp)\nsummary(logwlmm)\n## \n## Call:\n## lm(formula = TipSumLog ~ Weekend + AmountSum + Precipitation, \n##     data = tpp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.8616 -0.1996  0.1139  0.3116  0.5145 \n## \n## Coefficients:\n##                          Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)              2.339504   0.116325  20.112   &lt;2e-16 ***\n## WeekendWeekend           0.080292   0.114515   0.701    0.486    \n## AmountSum                0.003483   0.000284  12.265   &lt;2e-16 ***\n## PrecipitationLight Rain  0.044912   0.160070   0.281    0.780    \n## PrecipitationNo Rain    -0.022885   0.148774  -0.154    0.878    \n## PrecipitationHeavy Rain -0.095057   0.146373  -0.649    0.518    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4576 on 64 degrees of freedom\n## Multiple R-squared:  0.7281, Adjusted R-squared:  0.7069 \n## F-statistic: 34.28 on 5 and 64 DF,  p-value: &lt; 2.2e-16\nggplot(tpp, aes(x=TipSumLog, y = ..count..))+\n  geom_histogram(aes(fill = ..count..))+\n  scale_y_continuous(name = \"Count\") +\n  ggtitle(\"Frequency of Cash-outs amounts\")+\n  theme(legend.position = \"none\", panel.border = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),)+\n  geom_density(color=\"red\")\n\noutlierTest(logdlmm)\n##     rstudent unadjusted p-value Bonferroni p\n## 27 -5.370143         1.4486e-06    0.0001014\n## 53 -4.506757         3.2454e-05    0.0022718\noutlierTest(logwlmm)\n##     rstudent unadjusted p-value Bonferroni p\n## 27 -5.095281         3.4066e-06   0.00023846\n## 53 -4.164164         9.6757e-05   0.00677300\n#Heteroskedasticity\nncvTest(logdlmm)\n## Non-constant Variance Score Test \n## Variance formula: ~ fitted.values \n## Chisquare = 2.205876, Df = 1, p = 0.13749\nncvTest(logwlmm)\n## Non-constant Variance Score Test \n## Variance formula: ~ fitted.values \n## Chisquare = 1.472684, Df = 1, p = 0.22492\nA big problem I should’ve addressed first is outliers and skewness, AKA non-normality. Normality is the assumption that the error terms of a linear model are random, and do not follow any predictable pattern. Failing to account for predictable patterns in the distribution of error terms suggests that our regression model is missing some information about the relationship between the independent variable(s) and the outcome variable. This can happen when a few large outliers skew the distribution of observations. Just looking at the distribution of cash-outs, we can visually determine that our data is right skewed. To determine if there are any outliers, I used the Bonferroni Outlier Test, which uses a t-distribution to test whether the model’s largest studentized error terms outlier status are significantly different from other observations in the model. But before searching for any outliers, let’s look at our model after a log transformation. The log transformation will standardize the outcome variable, removing the natural skewness of our data set. Applying Bonferroni outlier test after the log transformation shows that observation 27 and 53 are outliers. This suggests our model violates the assumption of normality.\nMoving on, let’s test for heteroskedasticity. Heteroskedastcity is the violation of the homoscedasticity assumption, which is the assumption that the error terms is the same across observation, and does not dependent on the values of the independent variables. Heteroskedasticity refers to a systematic change in the variance of the residuals of the range of observations, i.e. non-constant variance. To diagnose non-constant variance in our observations, I used the non-constant variance score test on our log-transformed regression model, which determines if there is non-constant variance in our model. Any significant p-values would mean our data suffers from heteroskedasticity, but fortunately we do not see that issue. It’s safe to assume our model is homoscedastic."
  },
  {
    "objectID": "projects/Optimizing_Gratuitiy.html#section-iv-part-4.-multicollinearity",
    "href": "projects/Optimizing_Gratuitiy.html#section-iv-part-4.-multicollinearity",
    "title": "Optimizing Gratuity",
    "section": "Section IV, Part 4. Multicollinearity",
    "text": "Section IV, Part 4. Multicollinearity\nvif(logdlmm)\n##                   GVIF Df GVIF^(1/(2*Df))\n## `Day of Week` 1.325119  6        1.023736\n## AmountSum     1.164461  1        1.079102\n## Precipitation 1.205530  3        1.031644\nvif(logwlmm)\n##                   GVIF Df GVIF^(1/(2*Df))\n## Weekend       1.095035  1        1.046439\n## AmountSum     1.085621  1        1.041931\n## Precipitation 1.053302  3        1.008693\nThe fifth assumption of regression is that there is no perfect (multi)collinearity in our model. Multicolinearity is when two or more variables are highly linearly related. Even if our model is not perfectly multicollinearly related, imperfect or near multiconllinearity is still an issue because it can erroneously inflate variance. Although I have no theoretical reason to believe that any of our independent variables are perfect collinearities, or anywhere near perfect, to be safe I ran a Variance Inflation Factor test, which measures how much collinearity has increased the variance of our coefficients. As a rule of thumb, any VIF score above 5 is considered problematic for the regression model. Fortunately, I saw no evidence of multicollinearity in either of my regression models. It’s safe to assume our model has no perfect (or near perfect) multicollinearity."
  },
  {
    "objectID": "projects/Optimizing_Gratuitiy.html#section-iv-part-5.-correcting-assumptions-of-linearity",
    "href": "projects/Optimizing_Gratuitiy.html#section-iv-part-5.-correcting-assumptions-of-linearity",
    "title": "Optimizing Gratuity",
    "section": "Section IV, Part 5. Correcting Assumptions of Linearity",
    "text": "Section IV, Part 5. Correcting Assumptions of Linearity\ntppP &lt;- tpp[-c(53, 27), ]\n\nggplot(tppP, aes(x=TipSumLog, y = ..count..))+\n  geom_histogram(aes(fill = ..count..))+\n  scale_y_continuous(name = \"Count\") +\n  ggtitle(\"Frequency of Cash-outs amounts\")+\n  theme(legend.position = \"none\", panel.border = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),)+\n  geom_density(color=\"red\")\n\nlogdlmP &lt;- lm(TipSumLog ~ `Day of Week` + AmountSum + Precipitation + `Day of Week`*AmountSum, data=tppP)\nlogwlmP &lt;-lm(TipSumLog ~ Weekend + AmountSum + Precipitation + Weekend*AmountSum, data=tppP)\nsummary(logdlmP)\n## \n## Call:\n## lm(formula = TipSumLog ~ `Day of Week` + AmountSum + Precipitation + \n##     `Day of Week` * AmountSum, data = tppP)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.87542 -0.14852  0.00736  0.18074  0.62518 \n## \n## Coefficients:\n##                              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                 2.5461060  0.1481832  17.182  &lt; 2e-16 ***\n## `Day of Week`Mon           -0.3861386  0.2287637  -1.688 0.097530 .  \n## `Day of Week`Tue           -0.7910592  0.2363071  -3.348 0.001538 ** \n## `Day of Week`Wed           -0.4357577  0.2295328  -1.898 0.063302 .  \n## `Day of Week`Thu           -0.9374053  0.2186739  -4.287 8.06e-05 ***\n## `Day of Week`Sat            0.4065877  0.2044088   1.989 0.052068 .  \n## `Day of Week`Sun           -0.0589708  0.2234873  -0.264 0.792946    \n## AmountSum                   0.0027443  0.0003547   7.737 3.75e-10 ***\n## PrecipitationLight Rain     0.1152533  0.1136880   1.014 0.315479    \n## PrecipitationNo Rain        0.0226268  0.1050532   0.215 0.830327    \n## PrecipitationHeavy Rain    -0.0234668  0.0998079  -0.235 0.815058    \n## `Day of Week`Mon:AmountSum  0.0022023  0.0009333   2.360 0.022153 *  \n## `Day of Week`Tue:AmountSum  0.0039048  0.0009838   3.969 0.000226 ***\n## `Day of Week`Wed:AmountSum  0.0015419  0.0006378   2.417 0.019246 *  \n## `Day of Week`Thu:AmountSum  0.0033310  0.0008186   4.069 0.000164 ***\n## `Day of Week`Sat:AmountSum -0.0006871  0.0004949  -1.388 0.171036    \n## `Day of Week`Sun:AmountSum  0.0009261  0.0006879   1.346 0.184163    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2907 on 51 degrees of freedom\n## Multiple R-squared:  0.8974, Adjusted R-squared:  0.8652 \n## F-statistic: 27.88 on 16 and 51 DF,  p-value: &lt; 2.2e-16\nsummary(logwlmP)\n## \n## Call:\n## lm(formula = TipSumLog ~ Weekend + AmountSum + Precipitation + \n##     Weekend * AmountSum, data = tppP)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.86502 -0.15816  0.07118  0.21065  0.42562 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)               1.9868741  0.1072993  18.517  &lt; 2e-16 ***\n## WeekendWeekend            0.7235010  0.1320068   5.481 8.54e-07 ***\n## AmountSum                 0.0050728  0.0003922  12.934  &lt; 2e-16 ***\n## PrecipitationLight Rain   0.0772306  0.1154402   0.669    0.506    \n## PrecipitationNo Rain      0.0407318  0.1073184   0.380    0.706    \n## PrecipitationHeavy Rain  -0.0283925  0.1037984  -0.274    0.785    \n## WeekendWeekend:AmountSum -0.0025113  0.0004591  -5.470 8.89e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3209 on 61 degrees of freedom\n## Multiple R-squared:  0.8505, Adjusted R-squared:  0.8358 \n## F-statistic: 57.83 on 6 and 61 DF,  p-value: &lt; 2.2e-16\n(exp(coef(logwlmP)[\"WeekendWeekend\"]) - 1) * 100 \n## WeekendWeekend \n##       106.1638\nThe biggest issue in my model was non-normality. I fixed this by logarithmic transformation of my outcome variable to standardize the distribution of cash-outs, and then taking out the outliers. After removing those outliers, we see a much more normal distribution in our cash-outs. The last steps are to re-estimate our linear model including our interaction variable. We finally have 2 final linear models for both our hypotheses that do not violate the assumptions of linear regression.\nThe model for our first hypothesis didn’t see enough significant results.\nThe model for our second hypothesis did create significant results. Since we took the log transformation of our outcome variable, to interpret the result we must work backwards to interpret the log value of cash-outs on weekends, like so , which is equal to %106.16. Our linear model predicts that on weekends (Fridays, Saturdays and Sundays), we make %106.16 more tips than on weekdays."
  },
  {
    "objectID": "projects/Optimizing_Gratuitiy.html#footnotes",
    "href": "projects/Optimizing_Gratuitiy.html#footnotes",
    "title": "Optimizing Gratuity",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe pedantic reader will have noticed that I classified Friday as a weekend and consequently shook their head in disapproval. “Only Saturdays and Sundays are weekends. Fridays are weekdays.”, they might reproach. This traditional calendar taxonomy, garbed in an ontological gloss is at best a social convention, and certainly not an objective property of time itself (assuming time as such could have such properties). For the purposes of this study, it’s safe to reject this convention and replace it with one that is more practical for it’s ends. I work three days a week, so if I happen to demonstrate that weekends are better than weekdays in terms of tips, I can simply schedule myself to work on Friday, Saturday and Sunday. If the pendant persists in his pedantry, we need not use the terms “Weekend” and “Weekday”, we can instead simply refer to the set of days containing “Friday, Saturday and Sunday”, and the set containing “Monday, Tuesday, Wednesday and Thursday”.↩︎\nObservation 37 had a TipSum value of 0, which means I could not take the log of the variable without dealing with the zero. I decided to drop the observation because a $0 cash-out is very unlucky, and is almost certainly an outlier in our data set.↩︎"
  },
  {
    "objectID": "projects/Quiz-App.html",
    "href": "projects/Quiz-App.html",
    "title": "GUI-Based Quiz System",
    "section": "",
    "text": "This Python GUI project was orignally an assignment for COMP 218 at Athabasca University.\n\nGUI-Based Quiz System\n\n\n\n\nInterpretation\nGoal: develop a GUI program that allows the user to create quizzes, add questions to those quizzes, take the quizzes after adding questions and receive a final score at the end of quiz. The Quiz program should also be able to preview the quizzes after creating questions.\n\n\nAnalysis of tasks - required techniques and algorithms to accomplish the goal.\nQuiz and multiple choice question design:\n\nQuiz_item class models multiple choice questions, with a question, set of choices and correct answer.\n\nBesides a constructor, including methods to set the question, choices and answer after they’re created.\n\nQuiz class models a quiz. Uses Quiz_item objects as quiz questions.\n\nA method allows user to take the quiz, iterating through each quiz_item object in the quiz, displaying the question and the corresponding possible choices one by one. Allows the user to return the answer they believe to be correct.\nOnce the quiz is complete, the user’s total score as a percentage and any corrections are displayed.\nBesides a constructor and the quiz taking method, other methods include: add a quiz_item objects to the quiz and preview all quiz item questions, choices and answers in a quiz.\n\n\nGUI design and program logic:\n\nQuizApp class provides the GUI and methods for the program. Uses Tkinter module.\n\nMain Menu:\n\nCreate a new Quiz object, and add it to a dict of quiz name:quiz object pairs.\n\n\nUse a simple dialogue box to get user to input a name for the new quiz.\n\n\nSelect a quiz from the quizzes dictionary.\n\n\nDynamically create new buttons for each quiz in the quizzes dict. Buttons allow you to select a quiz to manage/execute.\n\nQuiz menu (upon selecting quiz)\n\nAdd quiz_item objects to selected quiz\n\nUse entry boxes to collect the quiz_item question and choices attributes\nUse Radiobuttons to select the correct choice as the answer attribute.\nValidate the proposed Quiz_item object, create it if valid, and add it to the selected Quiz object.\n\nPreview the question, choices and correct answer for each quiz_item object for the select quiz.\n\nIterate through the Quiz_items in the selected quiz, and return a formated string containing the question, choices and correct answer in a simple message box for each Quiz_item object.\nReturn a warning message box if the quiz has no quiz_items in it\n\nExecute the selected quiz after adding quiz_item objects to it.\n\nFirst checks if selected quiz object has any quiz items.\nPresents questions for user to select the correct answer to one question at a time.\nIncrement score variable for each correct answer\nStore quiz items answered correctly to find any corrections (if any) upon exiting the quiz.\n\nDynamically update a label for each question in the quiz, to show the quiz question.\nDynamically create buttons for each quiz question to allow user to pick an answer to the question.\n\nWhen taking the quiz, a countdown timer and a “Question [‘question number’] out of [‘total number of questions’]” label is visible to the user. If the countdown timer reaches zero, the user gets those questions wrong in the final score.\n\nUpdated every second\n\nQuiz exit conditions:\n\nThe number of questions answered by the user = the number of quiz_item objets in the quiz object; or,\nThe timer reaches 0.\n\n\n\nUpon exiting the quiz, display the final score as a percentage after the quiz is over, and all corrections if any.\n\n\nA method for clearing widgets from the parent window. Need to run this every time the user switches menus. Ran every time user changes menus, begins or finishes a quiz.\n“Return to previous menu” buttons should be available everywhere except for during the quiz.\n\n\n\n\nExplanation of modules, classes, and functions\n\nModules:\n\ntkinter package used to create the GUI components of the program.\n\nttk module for aesthetically pleasing GUI components\nsimpledialog and messagebox modules to open asktring and messagebox boxes for the user interface.\n\n\n\n\nClasses:\n\n\nQuiz_item - Models individual quiz items that can be added to quizzes.\nAttributes:\n\nquestion - The string containing the question.\nchoices - A variable length argument containing the possible answers to the question.\nanswer - The correct answer, validated to be one of the provided possible answers.\n\nMethods:\n\nset_question(new_question) - Updates the question description.\nset_choices(choice_number, new_choice) - Updates the specified possible choice.\nset_answer(new_answer) - Sets the correct answer, ensuring validity.\npreview() - Returns a string containing the question and choices of the Quiz_item objects.\n\n\n\nQuiz - Models Quizzes, acting like collections of Quiz_item objects with additional methods and functionality.\nAttributes:\n\nquizItemList - A list of Quiz_item objects.\ntitle - The title of the quiz.\n\nMethods:\n\nadd_question(quizItem) - Add Quiz_item object to the quizItemList.\ndisplay_questions() - Returns string of questions, choices and answers for GUI display.\ntake_quiz - Executes quiz on user, returns final score calculation and all corrections if any.\n\n\n\nQuizApp - Provides the GUI and methods for the program.\nAttributes:\n\nroot - the parent window for the GUI application (which is initialized in the main execution)\nquizzes - a dictionary containing quizzes.\nmain_menu_quizzes_frame - Frame widget containing main menu buttons to select quiz\ncurrent_quiz and current_quiz_name - The selected quiz object and its name\nanswer_var - Variable holding the user’s selected answer as an integer\nquestion_entry - the question of a new quiz item\nchoices_entries - list containing choices of a new quiz item\nscore - holds the score of as the user proceeds through a quiz\ncurrent_question_index - holds the index of the current Quiz_item objects in quizItemList as the user proceeds through the quiz\ntime_total - the total time the user has to take the quiz, in seconds\ntime_remaining - the time remaining the user has to complete the quiz, max = time_total and min = 0.\ntimer_running_flag - flag set to False by default, set to True when Timer widget is activated\nuser_got_right - list of questions the user got right when taking the quiz.\nquiz_complete_flag - flag set to False by default, set to true when Quiz complete.\ntimer - label widget acting as a countdown timer during the quiz.\nremaining_quiz_items - label widget listing the current question number and the total number of questions during the quiz.\nquiz_corrections_frame - Frame containing any corrections after the quiz is over.\n\nMethods:\n\nmain_menu() - Displays the main menu.\ncreate_quiz() - Creates new quiz object and adds it to the quizzes dictionary.\nquiz_menu(quiz_name) - Opens quiz menu for the selected quiz, allows user to add questions, preview questions and take the quiz.\nadd_question() - Menu with fields to create new quiz item object to be added to the selected quiz object.s\nsubmit_question() - Method to validate the quiz item object created in add_questions.\npreview_quiz() - Opens a dialogue box that previews all quiz items in the selected quiz object.\nstart_quiz() - Begins the interactive quiz program.\ndisplay_question() - Starting with the first quiz item object, displays the questions and all choices for the user to select as the answer.\ncheck_answer(user_answer) - When user selects an answer in display_question(), compares it to the correct answer of the question and updates the user’s score accordingly.\nupdate_timer() - Updates the timer to decrement by one every second. Also contains both exit conditions for the quiz: When the user answers every question and when the user runs out of time.\nshow_results() - Shows the user their score as a percentage and their corrections to incorrectly or unanswered questions, if any.\nclear_window() - Clears all widgets off the parent window. This runs every time the user changes menus, and whenever a quiz is started or finished.\n\n\n\n\nUser Guide\n\nMain Menu\n\nCreate a new Quiz by clicking “Create a Quiz”.\n\nCreate any number of quizes with unique names.\n\nSelect a created Quiz by clicking on the button with its name.\nClick “Exit” to close the program.\n\n\n\nQuiz Menu\n\nClick “Add Question” to add Quiz_item objects to the quiz.\n\nEnter Question, Choices, and select the correct answer using the Radiobuttons.\nClick “Submit” to finalize and validate your entries.\nClick “Cancel” to return to the Quiz Menu.\n\nAfter adding questions:\nClick “Preview Quiz” to preview all the quiz items in the currently selected Quiz.\nClick “Start Quiz” to begin taking the quiz.\nClick “Back to main Menu” to return to the main menu.\n\n\n\nTaking a Quiz\n\nRead the question, and click one of the buttons to select one of the choices as your answers.\nTimer and the number of remaining questions are visible at the bottom; make sure to manage your time.\n\nIf the timer reaches 0, all unaswered questions are marked as incorrect.\n\n\n\n\nQuiz Results\n\nYour score as a percentage, and corrections to any questions answered incorrectly (if any) are displayed.\n“Return to Main Menu” brings you to the main menu.\n\n\n\n\nCode\n# Importing dependencies\nfrom tkinter import *\nfrom tkinter.ttk import *\nfrom tkinter import simpledialog, messagebox\n\n# Define the Quiz_item class\nclass Quiz_item:\n    def __init__(self, question, *choices, answer): #4 attributes: the quiz item question, the possible choices and the correct answer\n        self.question = question\n        self.choices = list(choices)\n        if answer in self.choices: #making sure the provided answer is one of the provided choices...\n            self.answer = answer\n        else: #if not, raise ValueError\n            raise ValueError(\"The correct answer must be one of the choices\")\n\n    def set_question(self, new_question): #method to set the question\n        self.question = new_question\n\n    def set_choices(self, choice_number, new_choice): #method to set choices\n        self.choices[choice_number - 1] = new_choice\n\n    def set_answer(self, new_answer): #method to set the answer\n        if new_answer in self.choices: #making sure the provided answer is one of the provided choices...\n            self.answer = new_answer\n        else: #if not, raise value Error\n            raise ValueError(\"The correct answer must be one of the choices\")\n\n    def preview(self): #Method to return the question and choices (for the QuizApp class)\n        return f\"{self.question}\\n\" + \"\\n\".join(f\"{i+1}. {choice}\" for i, choice in enumerate(self.choices))\n    # returns a string containing the question, choices and the respective number of each choice (in the order provided)\n\n# Define the Quiz class\nclass Quiz: \n    def __init__(self, title): #2 attributes: a list to contain quiz items, a title to describe the quiz\n        self.quizItemList = []\n        self.title = title\n\n    def add_question(self, quizItem): #Method to add Quiz item to quiz\n        if isinstance(quizItem, Quiz_item): # add quiz item to quiz item list\n            self.quizItemList.append(quizItem)\n        else: # if the argument is not a quiz item, raise ValueError\n            raise ValueError(\"The argument must be a Quiz_Item object\")\n            \n    def display_questions(self): #method that returns questions, to be used by QuizApp class\n        return \"\\n\\n\".join(f\"Q{i+1}: {item.preview()}\\nAnswer: {item.answer}\" for i, item in enumerate(self.quizItemList))\n    \n    def take_quiz(self): #method to execute quiz on user\n        score = 0 #initalize score var\n        incorrect_answers = {} #initialize incorrect answers dict\n        for i in self.quizItemList: #for every quiz item object in the quizitemlist attribute,\n            print(i.question) #print the question\n            for j in i.choices: #print every choice\n                print(j)\n            user_answer = input(\"\") #accept user input\n            if user_answer == i.answer: #if the user input matches the answer:\n                score += 1 #increment score by 1\n            else: #else,\n                incorrect_answers.update({i.question:i.answer})# add the question:correct answer pair to the dict\n        print(f'You scored: {score/len(self.quizItemList)*100}%') #print final score as a percentage\n        for key, value in incorrect_answers.items(): #if any incorrect answers\n            print(f'Corrections: {key} - {value} ') #print the questio and correct answer\n\n# GUI System\nclass QuizApp:\n    def __init__(self, root): #Takes the parent window as arg\n        self.root = root #parent window\n        self.quizzes = {} #intialize empty dict for quizzes\n        self.main_menu() #load main menu widgets\n\n    def main_menu(self): #method to load main menu features\n        self.clear_window() #clear all preview widgets (necessary when navigating to the main menu from any other menu)\n\n        Label(self.root, text=\"Saumi's Quiz App\", font=(\"Arial\", 20)).pack(pady=10) # Title label\n        \n        Button(self.root, text=\"Create a Quiz\", command=self.create_quiz).pack(pady=5) #Create Quiz Button, runs create_quiz on press\n        Label(self.root, text=\"Quizzes\", font=(\"Arial\", 14)).pack(pady=10) # \"Quizzes\" label; quizzes will land under this label when created\n        \n        #Frame for quiz_menu buttons\n        self.main_menu_quizzes_frame = Frame(self.root)\n        self.main_menu_quizzes_frame.pack(pady=10)\n        for count, quiz_name in enumerate(self.quizzes):\n            Button(self.main_menu_quizzes_frame, text=quiz_name, command=lambda name=quiz_name: self.quiz_menu(name)).grid(row = count//3, column= count % 3)  \n            # Iteratively create a button for each quiz; opens the respective quiz's quiz menu\n            # Lambda definition of quiz_name=name ensures the name argument for quiz_menu does not get overwritten when creating multiple quizzes\n            # eacch button organized in rows of three\n            \n        Button(self.root, text=\"Exit\", command=self.root.destroy).pack(pady=20,side=\"bottom\") #exit button, quits program\n\n    def create_quiz(self): #method to add create a new quiz\n        quiz_name = simpledialog.askstring(\"Create Quiz\", \"Enter the name of the quiz:\") #opens a dialogue box for user to input quiz name\n        if quiz_name: #as long as the user inputed non-emptry string (e.g. did not hit click cancel)\n            self.quizzes[quiz_name] = Quiz(quiz_name) #updates the quizzes dictionary with {quiz_name:Quiz-object} pair\n            messagebox.showinfo(\"Success\", f\"Quiz '{quiz_name}' created!\") #gives user success message when new quiz is created\n            self.main_menu() #reload main menu to update quiz_menu buttons\n\n    def quiz_menu(self, quiz_name): #when a quiz button is clicked on the main menu:\n        self.current_quiz = self.quizzes[quiz_name] #store the selected Quiz object\n        self.current_quiz_name = quiz_name #store its name\n        self.clear_window() #clear windows\n    \n        Label(self.root, text=f\"Quiz: {quiz_name}\", font=(\"Arial\", 16)).pack(pady=10) #Label: \"Quiz: [Quiz name]\"\n        Button(self.root, text=\"Add Question\", command=self.add_question).pack(pady=5) #Add question button; runs add_question to add new quiz item\n        Button(self.root, text=\"Preview Quiz\", command=self.preview_quiz).pack(pady=5) #Preview Quiz button; runs preview_quiz to display attributes of quiz items in current Quiz objecta\n        Button(self.root, text=\"Start Quiz\", command=self.start_quiz).pack(pady=5) #Start Quiz button; runs start_quiz to take the quiz after adding questions\n        Button(self.root, text=\"Back to Main menu\", command=self.main_menu).pack(pady=20, side=\"bottom\") #Return to main menu button; runs main_menu\n\n    def add_question(self): #When Add Question is clicked in quiz_menu:\n        self.clear_window() #clear windows\n\n        self.answer_var = IntVar() #Initialize IntVar for the selected answer\n\n        # Question Label and Entry\n        Label(self.root, text=\"Enter Question:\", font=(\"Arial\", 12)).grid(row=0, column=0, columnspan=2, padx =10, pady=10, sticky=\"w\")\n        self.question_entry = Entry(self.root, width=50) #entry box to enter the quiz question\n        self.question_entry.grid(row=1, column=0, columnspan=2, padx=10, pady=5)\n        Label(self.root, text=\"Enter Choices, and select correct answer:\", font=(\"Arial\", 12)).grid(row=2, column=0, columnspan=2, padx = 10, pady=10, sticky=\"w\")\n\n        # Choices Entry and Radio Buttons\n        self.choice_entries = [] #intialize list to contain choices entered by user\n        for i in range(4): #Create 4 entry boxes to enter choices to the question, and 4 respective radiobuttons to select the correct choice\n            Radiobutton(self.root, variable=self.answer_var, value=i - 1).grid(row=3 + i, column=0, padx=5, pady=5, sticky=\"w\")\n            choice_entry = Entry(self.root, width=40)\n            choice_entry.grid(row=3 + i, column=1, padx=10, pady=5, sticky=\"w\")\n            self.choice_entries.append(choice_entry) #Appends each entered choice to choice_entries attribute\n\n        # Submit and Cancel Buttons\n        Button(self.root, text=\"Submit\", command=self.submit_question).grid(row=7, column=0, padx=10, pady=20, sticky=\"e\") #submit button; runs submit_question to save entered quiz item data to the currentQuiz object\n        Button(self.root, text=\"Cancel\", command= lambda: self.quiz_menu(self.current_quiz_name)).grid(row=7, column=1, padx=10, pady=20, sticky=\"w\")\n        # cancel button, returns user to the quiz menu for the selected quiz\n\n    def submit_question(self): #When submit button is clicked in add question menu:\n        question = self.question_entry.get() #return and store the entered question text\n        choices = [entry.get() for entry in self.choice_entries] #return and store the entered choices text in for each choice in the list\n        correct_choice_index = self.answer_var.get() + 1 #return and store the correct choice number \n\n        # Validation\n        if not question.strip():\n            messagebox.showerror(\"Error\", \"Question cannot be empty!\") #Open message box if question entry box is empty\n            return\n        if not all(choices) or len(set(choices)) &lt; len(choices): #if any of the choice entry boxxes are empty or repeats:\n            messagebox.showerror(\"Error\", \"All choices must be filled and unique!\") # show error message box\n            return\n        if correct_choice_index &lt; 0 or correct_choice_index &gt;= len(choices): #if no correct choice radiobutton is ticked:\n            messagebox.showerror(\"Error\", \"Please select a valid correct answer!\") # show error message box\n            return\n\n        # Add the question to the quiz\n        self.current_quiz.add_question(Quiz_item(question, *choices, answer=choices[correct_choice_index])) #user quiz_item and Quiz methods to add quiz item to the selected quiz\n        messagebox.showinfo(\"Success\", \"Question added!\") # open success message\n\n        #return to selected quiz quiz menu\n        self.quiz_menu(self.current_quiz_name)\n\n    def preview_quiz(self): #when preview quiz button is clicked on quiz menu:\n        preview = self.current_quiz.display_questions() #runs the display_questions method of the Quiz class, save what the returned string\n        if preview: #if the preview string is not empty:\n            messagebox.showinfo(\"Preview Quiz\", preview) #open a messagebox showing all the quiz items for the selected quiz\n        else: #if the preview string is empty, no quiz items have been added to the selected quiz object:\n            messagebox.showinfo(\"Preview Quiz\", \"No questions added yet!\") #warn user of this\n\n    def start_quiz(self): #when Start Quiz button is clicked on quiz menu:\n        if not self.current_quiz.quizItemList: #if current quiz object has no quiz items:\n            messagebox.showerror(\"Error\", \"No questions in the quiz!\") #return error box\n            return\n\n        #Initializing variables\n        self.score = 0\n        self.current_question_index = 0\n        self.time_total = 60 #time to take quiz in seconds\n        self.time_remaining  = self.time_total\n        self.timer_running_flag = False\n        self.user_got_right = []\n        self.quiz_complete_flag = False\n\n        #run display_question\n        self.display_question()\n\n    def display_question(self): # ran after start quiz, and after each time a user answers a question in the quiz\n        self.clear_window() #clear window\n        \n        #quiz complete exit condition: when user answers all questions\n        if self.current_question_index == len(self.current_quiz.quizItemList):\n            self.quiz_complete_flag = True #set quiz complete flag to True to indicate the quiz was completed\n            print(\"Quiz complete quiz_complete_flag triggered\") #terminal message to monitor quiz functionality\n            return\n        \n        #Quiz questions label\n        quiz_item = self.current_quiz.quizItemList[self.current_question_index] #In the order they were created, save the current quiz item in the current quiz\n        Label(self.root, text=quiz_item.question, font=(\"Arial\", 14)).pack(pady=10, expand=True) #label displaying the question attribute of the current quiz item\n\n        #Quiz choices buttons\n        for i, choice in enumerate(quiz_item.choices): #for each choice in the current quiz item object, create a button to select that choice as the answer\n            Button(self.root, text=choice, command= lambda c=choice : self.check_answer(c)).pack(pady=5)\n            #when clicked run check_answer to update the user's score\n\n        self.current_question_index += 1 #increment to the next quiz item object (in the order they were created)\n\n        # Timer label\n        self.timer = Label(self.root, text=f\"Time: {self.time_remaining}/{self.time_total}s\", font=\"Arial\")\n        self.timer.pack(pady=10)\n        if self.timer_running_flag == False: #this test ensures only one instance of update_timer is called\n            self.timer_running_flag = True\n            self.update_timer()\n\n        # Quiz items remaining label\n        self.remaining_quiz_items = Label(self.root, text=f\"Question: {self.current_question_index}/{len(self.current_quiz.quizItemList)}\") #remaining questions label below timer: \"Question: [current quiz number]/[lenght of quiz]\"\n        self.remaining_quiz_items.pack(pady=5)\n\n    def check_answer(self, user_answer): #ran after user selects a choice during the quiz\n        quiz_item = self.current_quiz.quizItemList[self.current_question_index - 1] #take the current quiz item of the current quiz object\n        if user_answer == quiz_item.answer: #if the user's answer matches the answer attribute\n            self.score += 1 #increment the score attribute\n            self.user_got_right.append(quiz_item) #append the quiz item to a list of questions the user answered correctly\n        self.display_question() #rerun display question to go to next question/end quiz\n\n    def update_timer(self): #runs the first time display_question is called\n        if self.time_remaining == 0 and self.quiz_complete_flag == False: #timeout exit condition; when the timer reaches 0 without completing every question\n            print(\"Quiz timeout exit\") #progress tracking terminal message\n            return self.show_results() #quiz finished; call show results to finish quiz\n        if self.time_remaining &gt; 0 and self.quiz_complete_flag == False: #when timer is above 0, and user has not completed the quiz\n            self.time_remaining -= 1 #decrement time remaining lavariable bel by 1\n            self.timer.config(text=f\"Time: {self.time_remaining}/{self.time_total}s\", font=\"Arial\") #update the timer label to reflect this change in time\n            self.root.after(1000, self.update_timer) #after a delay of 1000ms (i.e. 1 second), rerun update timer\n        elif self.quiz_complete_flag == True: #when quiz complete exit condition is met,\n            print(\"Quiz complete exit\") #print progress tracking terminal message\n            return self.show_results() #quiz finished; call show results to finish quiz\n\n    def show_results(self): #called when quiz is complete\n        self.clear_window() #clear window\n\n        percentage = (self.score / len(self.current_quiz.quizItemList)) * 100 #calculate user's score as a percentage\n        Label(self.root, text=f\"Your Score: {round(percentage, 2)}%\", font=(\"Arial\", 16)).pack(pady=10) #label dispalying user score\n        user_got_wrong = [items for items in self.current_quiz.quizItemList if items not in self.user_got_right] #find the complement of the set of correctly answered quiz items; quiz items either answered incorrectly or unanswered due to timeout.\n        Label(self.root, text=\"Corrections\", font=(\"Arial\", 14)).pack(pady=10) #Corrections header label\n        self.quiz_corrections_frame = Frame(self.root) #frame to contain corrections if any\n        self.quiz_corrections_frame.pack(pady=3)\n        if user_got_wrong: #if the list of items the user missed or answered incorrectly is not empty:\n            for i in range(len(user_got_wrong)): #for the number of items in the length of the user_got_wrong list:\n                Label(self.quiz_corrections_frame, text=f'Question: {user_got_wrong[i].question}\\nAnswer: {user_got_wrong[i].answer}').grid(row = i//3, column= i % 3, padx=5, pady=5)\n                # print a label containing the quiz item question and correct answer of the quiz items the user missed\n            \n            \n        #exit button to return to main menu\n        Button(self.root, text=\"Back to Main Menu\", command=self.main_menu).pack(pady=20,side=\"bottom\")\n\n    def clear_window(self): #this function runs everytime the window needs to change what its displaying (i.e., every menu change, and every question answered)\n        for widget in self.root.winfo_children(): #for widgets in the parent window:\n            widget.destroy() #remove widgets\n\n\n\n\n# Main Execution\nroot = Tk() #parent window\nroot.title(\"Quiz System\") # parent window title\nroot.geometry(\"800x700\") # size of parent window\nquiz_system = QuizApp(root) #creating instance of QuizApp\nroot.mainloop() #executing the program"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Saumi Rahnamay",
    "section": "",
    "text": "I graduated from Simon Fraser University in 2023 with a Bachelor of Arts and Social Sciences in Honours Philosophy, with a Minor in Social Data Analytics. I’m currently a masters student at UBC studying Computational Linguistics in Data Science. I expect to graduate in June 2026.\nYou can find my most up to date CV here."
  },
  {
    "objectID": "index.html#hi-im-saumi.",
    "href": "index.html#hi-im-saumi.",
    "title": "Saumi Rahnamay",
    "section": "",
    "text": "I graduated from Simon Fraser University in 2023 with a Bachelor of Arts and Social Sciences in Honours Philosophy, with a Minor in Social Data Analytics. I’m currently a masters student at UBC studying Computational Linguistics in Data Science. I expect to graduate in June 2026.\nYou can find my most up to date CV here."
  },
  {
    "objectID": "index.html#featured-work",
    "href": "index.html#featured-work",
    "title": "Saumi Rahnamay",
    "section": "Featured Work",
    "text": "Featured Work\n\n\n\n\nOptimizing Gratuity\nAnalysis of day-of-week and precipitation levels on tip amounts from my part-time pizza delivery job.\n\nDemonstrated a 106.16% increase in expected tips; demonstrated the effect of preset tip percentage options on expected tip.\nInferential statistics performed with R.\n\n\n\n\nSuper Smash Brothers Player Statistics\nAnalysis of my own player statistics in the fighting game Super Smash Brothers Melee.\nDetermining where my play needs improvement, and optimizing my tournament strategy.\nEDA performed in SQL.\n\n\n\n\nForthcoming ML Project…\n\nI am enrolled in several ML courses at UBC and am applying what I’m learning to a Machine Learning/Computational Linguistics project.\nIn the meantime, I encourage you to imagine an impressive ML project here.\n\n\n\n\n\nView all projects →"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to Saumi’s domain. Please enjoy your stay."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "Most Streamed Spotify Songs: Data Cleaning\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUI-Based Quiz System\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing Desirability\n\n\n\nGIS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing Gratuity\n\n\n\nR\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuper Smash Brothers Player Statistics\n\n\n\nSQL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRAship Code\n\n\n\nR\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "",
    "section": "",
    "text": "Creative Commons Legal Code\nCC0 1.0 Universal\nCREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE\nLEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN\nATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS\nINFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES\nREGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS\nPROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM\nTHE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED\nHEREUNDER.\nStatement of Purpose\nThe laws of most jurisdictions throughout the world automatically confer exclusive Copyright and Related Rights (defined below) upon the creator and subsequent owner(s) (each and all, an “owner”) of an original work of authorship and/or a database (each, a “Work”).\nCertain owners wish to permanently relinquish those rights to a Work for the purpose of contributing to a commons of creative, cultural and scientific works (“Commons”) that the public can reliably and without fear of later claims of infringement build upon, modify, incorporate in other works, reuse and redistribute as freely as possible in any form whatsoever and for any purposes, including without limitation commercial purposes. These owners may contribute to the Commons to promote the ideal of a free culture and the further production of creative, cultural and scientific works, or to gain reputation or greater distribution for their Work in part through the use and efforts of others.\nFor these and/or other purposes and motivations, and without any expectation of additional consideration or compensation, the person associating CC0 with a Work (the “Affirmer”), to the extent that he or she is an owner of Copyright and Related Rights in the Work, voluntarily elects to apply CC0 to the Work and publicly distribute the Work under its terms, with knowledge of his or her Copyright and Related Rights in the Work and the meaning and intended legal effect of CC0 on those rights.\n\nCopyright and Related Rights. A Work made available under CC0 may be protected by copyright and related or neighboring rights (“Copyright and Related Rights”). Copyright and Related Rights include, but are not limited to, the following:\n\n\nthe right to reproduce, adapt, distribute, perform, display, communicate, and translate a Work;\nmoral rights retained by the original author(s) and/or performer(s);\npublicity and privacy rights pertaining to a person’s image or likeness depicted in a Work;\nrights protecting against unfair competition in regards to a Work, subject to the limitations in paragraph 4(a), below;\nrights protecting the extraction, dissemination, use and reuse of data in a Work;\ndatabase rights (such as those arising under Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, and under any national implementation thereof, including any amended or successor version of such directive); and\nother similar, equivalent or corresponding rights throughout the world based on applicable law or treaty, and any national implementations thereof.\n\n\nWaiver. To the greatest extent permitted by, but not in contravention of, applicable law, Affirmer hereby overtly, fully, permanently, irrevocably and unconditionally waives, abandons, and surrenders all of Affirmer’s Copyright and Related Rights and associated claims and causes of action, whether now known or unknown (including existing as well as future claims and causes of action), in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the “Waiver”). Affirmer makes the Waiver for the benefit of each member of the public at large and to the detriment of Affirmer’s heirs and successors, fully intending that such Waiver shall not be subject to revocation, rescission, cancellation, termination, or any other legal or equitable action to disrupt the quiet enjoyment of the Work by the public as contemplated by Affirmer’s express Statement of Purpose.\nPublic License Fallback. Should any part of the Waiver for any reason be judged legally invalid or ineffective under applicable law, then the Waiver shall be preserved to the maximum extent permitted taking into account Affirmer’s express Statement of Purpose. In addition, to the extent the Waiver is so judged Affirmer hereby grants to each affected person a royalty-free, non transferable, non sublicensable, non exclusive, irrevocable and unconditional license to exercise Affirmer’s Copyright and Related Rights in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the “License”). The License shall be deemed effective as of the date CC0 was applied by Affirmer to the Work. Should any part of the License for any reason be judged legally invalid or ineffective under applicable law, such partial invalidity or ineffectiveness shall not invalidate the remainder of the License, and in such case Affirmer hereby affirms that he or she will not (i) exercise any of his or her remaining Copyright and Related Rights in the Work or (ii) assert any associated claims and causes of action with respect to the Work, in either case contrary to Affirmer’s express Statement of Purpose.\nLimitations and Disclaimers.\n\n\nNo trademark or patent rights held by Affirmer are waived, abandoned, surrendered, licensed or otherwise affected by this document.\nAffirmer offers the Work as-is and makes no representations or warranties of any kind concerning the Work, express, implied, statutory or otherwise, including without limitation warranties of title, merchantability, fitness for a particular purpose, non infringement, or the absence of latent or other defects, accuracy, or the present or absence of errors, whether or not discoverable, all to the greatest extent permissible under applicable law.\nAffirmer disclaims responsibility for clearing rights of other persons that may apply to the Work or any use thereof, including without limitation any person’s Copyright and Related Rights in the Work. Further, Affirmer disclaims responsibility for obtaining any necessary consents, permissions or other rights required for any use of the Work.\nAffirmer understands and acknowledges that Creative Commons is not a party to this document and has no duty or obligation with respect to this CC0 or use of the Work."
  },
  {
    "objectID": "projects/most_streamed_spotify_jn.html",
    "href": "projects/most_streamed_spotify_jn.html",
    "title": "Most Streamed Spotify Songs: Data Cleaning",
    "section": "",
    "text": "This is a simple data cleaning + EDA project. I used this data set from Kaggle.\nimport numpy as np\nimport pandas as pd\nimport re"
  },
  {
    "objectID": "projects/most_streamed_spotify_jn.html#data-cleaning",
    "href": "projects/most_streamed_spotify_jn.html#data-cleaning",
    "title": "Most Streamed Spotify Songs: Data Cleaning",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nCorrect encoding\nData Types\nMissing values\nDuplicates\nOutliers\nFormating Data (Date)\n\n\n1. Correct encoding\n\nsdf = pd.read_csv(\"Most Streamed Spotify Songs 2024.csv\", encoding='ISO-8859-1') # not encoded in utf-8\nsdf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4600 entries, 0 to 4599\nData columns (total 29 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Track                       4600 non-null   object \n 1   Album Name                  4600 non-null   object \n 2   Artist                      4595 non-null   object \n 3   Release Date                4600 non-null   object \n 4   ISRC                        4600 non-null   object \n 5   All Time Rank               4600 non-null   object \n 6   Track Score                 4600 non-null   float64\n 7   Spotify Streams             4487 non-null   object \n 8   Spotify Playlist Count      4530 non-null   object \n 9   Spotify Playlist Reach      4528 non-null   object \n 10  Spotify Popularity          3796 non-null   float64\n 11  YouTube Views               4292 non-null   object \n 12  YouTube Likes               4285 non-null   object \n 13  TikTok Posts                3427 non-null   object \n 14  TikTok Likes                3620 non-null   object \n 15  TikTok Views                3619 non-null   object \n 16  YouTube Playlist Reach      3591 non-null   object \n 17  Apple Music Playlist Count  4039 non-null   float64\n 18  AirPlay Spins               4102 non-null   object \n 19  SiriusXM Spins              2477 non-null   object \n 20  Deezer Playlist Count       3679 non-null   float64\n 21  Deezer Playlist Reach       3672 non-null   object \n 22  Amazon Playlist Count       3545 non-null   float64\n 23  Pandora Streams             3494 non-null   object \n 24  Pandora Track Stations      3332 non-null   object \n 25  Soundcloud Streams          1267 non-null   object \n 26  Shazam Counts               4023 non-null   object \n 27  TIDAL Popularity            0 non-null      float64\n 28  Explicit Track              4600 non-null   int64  \ndtypes: float64(6), int64(1), object(22)\nmemory usage: 1.0+ MB\n\n\n\n\n2. Data Types\nMost numerical variables were erroneously encoded as strings. Correcting this here.\n\n# selecting all numerical variables erroneously encoded as a string object.\nobjCol = sdf.select_dtypes(include=[object]).drop(['Track','Album Name','Artist','Release Date','ISRC'], axis=1).columns\n\n\ndef convert_to_int(value, default=None):\n    try:\n        # Use regex to remove non-numeric characters\n        cleaned_value = re.sub(r'[^\\d]', '', str(value))\n        result = int(cleaned_value)\n        return result\n    except (ValueError, TypeError):\n        return default\nsdf[objCol] = sdf[objCol].map(convert_to_int)\n\n\nsdf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4600 entries, 0 to 4599\nData columns (total 29 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Track                       4600 non-null   object \n 1   Album Name                  4600 non-null   object \n 2   Artist                      4595 non-null   object \n 3   Release Date                4600 non-null   object \n 4   ISRC                        4600 non-null   object \n 5   All Time Rank               4600 non-null   int64  \n 6   Track Score                 4600 non-null   float64\n 7   Spotify Streams             4487 non-null   float64\n 8   Spotify Playlist Count      4530 non-null   float64\n 9   Spotify Playlist Reach      4528 non-null   float64\n 10  Spotify Popularity          3796 non-null   float64\n 11  YouTube Views               4292 non-null   float64\n 12  YouTube Likes               4285 non-null   float64\n 13  TikTok Posts                3427 non-null   float64\n 14  TikTok Likes                3620 non-null   float64\n 15  TikTok Views                3619 non-null   float64\n 16  YouTube Playlist Reach      3591 non-null   float64\n 17  Apple Music Playlist Count  4039 non-null   float64\n 18  AirPlay Spins               4102 non-null   float64\n 19  SiriusXM Spins              2477 non-null   float64\n 20  Deezer Playlist Count       3679 non-null   float64\n 21  Deezer Playlist Reach       3672 non-null   float64\n 22  Amazon Playlist Count       3545 non-null   float64\n 23  Pandora Streams             3494 non-null   float64\n 24  Pandora Track Stations      3332 non-null   float64\n 25  Soundcloud Streams          1267 non-null   float64\n 26  Shazam Counts               4023 non-null   float64\n 27  TIDAL Popularity            0 non-null      float64\n 28  Explicit Track              4600 non-null   int64  \ndtypes: float64(22), int64(2), object(5)\nmemory usage: 1.0+ MB\n\n\n\n\n3. Missing Values\n\nGet a list of the count of missing values in each column, and drop the ones with too many.\nImpute missing numerical values with mean\n\n\nsdfNAs =  pd.DataFrame(sdf.isnull().sum()).reset_index().rename(columns={'index':'Track', 0 :'Missing Values'})\nsdfNAs[sdfNAs['Missing Values']&gt;0].sort_values(by='Missing Values', ascending=False)\n\n\n\n\n\n\n\n\nTrack\nMissing Values\n\n\n\n\n27\nTIDAL Popularity\n4600\n\n\n25\nSoundcloud Streams\n3333\n\n\n19\nSiriusXM Spins\n2123\n\n\n24\nPandora Track Stations\n1268\n\n\n13\nTikTok Posts\n1173\n\n\n23\nPandora Streams\n1106\n\n\n22\nAmazon Playlist Count\n1055\n\n\n16\nYouTube Playlist Reach\n1009\n\n\n15\nTikTok Views\n981\n\n\n14\nTikTok Likes\n980\n\n\n21\nDeezer Playlist Reach\n928\n\n\n20\nDeezer Playlist Count\n921\n\n\n10\nSpotify Popularity\n804\n\n\n26\nShazam Counts\n577\n\n\n17\nApple Music Playlist Count\n561\n\n\n18\nAirPlay Spins\n498\n\n\n12\nYouTube Likes\n315\n\n\n11\nYouTube Views\n308\n\n\n7\nSpotify Streams\n113\n\n\n9\nSpotify Playlist Reach\n72\n\n\n8\nSpotify Playlist Count\n70\n\n\n2\nArtist\n5\n\n\n\n\n\n\n\nChoosing to drop “TIDAL Popularity”, “Soundcloud Streams”, “SiriusXM Spins” because they are: 1. missing too many values, and; 2. not incredibly interesting.\n\nsdf.drop([\"TIDAL Popularity\", \"Soundcloud Streams\", \"SiriusXM Spins\"], axis=1, inplace=True)\nsdf.head()\n\n\n\n\n\n\n\n\nTrack\nAlbum Name\nArtist\nRelease Date\nISRC\nAll Time Rank\nTrack Score\nSpotify Streams\nSpotify Playlist Count\nSpotify Playlist Reach\n...\nYouTube Playlist Reach\nApple Music Playlist Count\nAirPlay Spins\nDeezer Playlist Count\nDeezer Playlist Reach\nAmazon Playlist Count\nPandora Streams\nPandora Track Stations\nShazam Counts\nExplicit Track\n\n\n\n\n0\nMILLION DOLLAR BABY\nMillion Dollar Baby - Single\nTommy Richman\n4/26/2024\nQM24S2402528\n1\n725.4\n3.904709e+08\n30716.0\n196631588.0\n...\n1.505970e+08\n210.0\n40975.0\n62.0\n17598718.0\n114.0\n18004655.0\n22931.0\n2669262.0\n0\n\n\n1\nNot Like Us\nNot Like Us\nKendrick Lamar\n5/4/2024\nUSUG12400910\n2\n545.9\n3.237039e+08\n28113.0\n174597137.0\n...\n1.563804e+08\n188.0\n40778.0\n67.0\n10422430.0\n111.0\n7780028.0\n28444.0\n1118279.0\n1\n\n\n2\ni like the way you kiss me\nI like the way you kiss me\nArtemas\n3/19/2024\nQZJ842400387\n3\n538.4\n6.013093e+08\n54331.0\n211607669.0\n...\n3.737850e+08\n190.0\n74333.0\n136.0\n36321847.0\n172.0\n5022621.0\n5639.0\n5285340.0\n0\n\n\n3\nFlowers\nFlowers - Single\nMiley Cyrus\n1/12/2023\nUSSM12209777\n4\n444.9\n2.031281e+09\n269802.0\n136569078.0\n...\n3.351189e+09\n394.0\n1474799.0\n264.0\n24684248.0\n210.0\n190260277.0\n203384.0\n11822942.0\n0\n\n\n4\nHoudini\nHoudini\nEminem\n5/31/2024\nUSUG12403398\n5\n423.3\n1.070349e+08\n7223.0\n151469874.0\n...\n1.127639e+08\n182.0\n12185.0\n82.0\n17660624.0\n105.0\n4493884.0\n7006.0\n457017.0\n1\n\n\n\n\n5 rows × 26 columns\n\n\n\nImputing missing numerical values by mean.\n\nintCol = sdf.select_dtypes(include=[np.number]).columns\n\nsdf[intCol] = sdf[intCol].fillna(sdf[intCol].mean())\nintCol\n\nIndex(['All Time Rank', 'Track Score', 'Spotify Streams',\n       'Spotify Playlist Count', 'Spotify Playlist Reach',\n       'Spotify Popularity', 'YouTube Views', 'YouTube Likes', 'TikTok Posts',\n       'TikTok Likes', 'TikTok Views', 'YouTube Playlist Reach',\n       'Apple Music Playlist Count', 'AirPlay Spins', 'Deezer Playlist Count',\n       'Deezer Playlist Reach', 'Amazon Playlist Count', 'Pandora Streams',\n       'Pandora Track Stations', 'Shazam Counts', 'Explicit Track'],\n      dtype='object')\n\n\n\nsdfNAs =  pd.DataFrame(sdf.isnull().sum()).reset_index().rename(columns={'index':'Track', 0 :'Missing Values'})\nsdfNAs[sdfNAs['Missing Values']&gt;0].sort_values(by='Missing Values', ascending=False)\n\n\n\n\n\n\n\n\nTrack\nMissing Values\n\n\n\n\n2\nArtist\n5\n\n\n\n\n\n\n\n\n\n4. Duplicates\n\nsdfDupes = pd.DataFrame(sdf.duplicated()).rename(columns={0:'duplicate'})\nsdfDupes[sdfDupes['duplicate']== True]\n\n\n\n\n\n\n\n\nduplicate\n\n\n\n\n2450\nTrue\n\n\n3450\nTrue\n\n\n\n\n\n\n\n\nsdf.iloc[2449:2451]\n\n\n\n\n\n\n\n\nTrack\nAlbum Name\nArtist\nRelease Date\nISRC\nAll Time Rank\nTrack Score\nSpotify Streams\nSpotify Playlist Count\nSpotify Playlist Reach\n...\nYouTube Playlist Reach\nApple Music Playlist Count\nAirPlay Spins\nDeezer Playlist Count\nDeezer Playlist Reach\nAmazon Playlist Count\nPandora Streams\nPandora Track Stations\nShazam Counts\nExplicit Track\n\n\n\n\n2449\nTennessee Orange\nTennessee Orange\nMegan Moroney\n9/2/2022\nTCAGJ2289254\n2424\n28.9\n227893586.0\n28139.0\n12480714.0\n...\n238206228.0\n33.0\n129172.0\n5.0\n1370.0\n49.0\n56972562.0\n26968.0\n708143.0\n0\n\n\n2450\nTennessee Orange\nTennessee Orange\nMegan Moroney\n9/2/2022\nTCAGJ2289254\n2424\n28.9\n227893586.0\n28139.0\n12480714.0\n...\n238206228.0\n33.0\n129172.0\n5.0\n1370.0\n49.0\n56972562.0\n26968.0\n708143.0\n0\n\n\n\n\n2 rows × 26 columns\n\n\n\n\nsdf = sdf.drop(sdf.index[2449], axis=0).reset_index()\nsdf.iloc[2448:2452]\n\n\n\n\n\n\n\n\nindex\nTrack\nAlbum Name\nArtist\nRelease Date\nISRC\nAll Time Rank\nTrack Score\nSpotify Streams\nSpotify Playlist Count\n...\nYouTube Playlist Reach\nApple Music Playlist Count\nAirPlay Spins\nDeezer Playlist Count\nDeezer Playlist Reach\nAmazon Playlist Count\nPandora Streams\nPandora Track Stations\nShazam Counts\nExplicit Track\n\n\n\n\n2448\n2448\nAm I Wrong\nMore Music 2013\nNico & Vinz\n1/1/2013\nUSWB11304681\n2440\n28.9\n965827790.0\n160031.0\n...\n2936860.0\n89.0\n31268.0\n64.0\n111728.0\n13.0\n598857243.0\n319806.0\n19231875.0\n0\n\n\n2449\n2450\nTennessee Orange\nTennessee Orange\nMegan Moroney\n9/2/2022\nTCAGJ2289254\n2424\n28.9\n227893586.0\n28139.0\n...\n238206228.0\n33.0\n129172.0\n5.0\n1370.0\n49.0\n56972562.0\n26968.0\n708143.0\n0\n\n\n2450\n2451\nStill Trappin' (with King Von)\nThe Voice\nLil Durk\n12/24/2020\nUSUM72024651\n2448\n28.8\n176140377.0\n76081.0\n...\n35450506.0\n36.0\n10226.0\n7.0\n31768.0\n13.0\n27955730.0\n8002.0\n494233.0\n1\n\n\n2451\n2452\nDe Fresa y Coco\nDe Fresa y Coco\nLuis R Conriquez\n11/10/2023\nQMANG2226771\n2445\n28.8\n81186913.0\n7818.0\n...\n242836946.0\n3.0\n3.0\n1.0\n2878.0\n1.0\n340543.0\n2169.0\n147308.0\n0\n\n\n\n\n4 rows × 27 columns\n\n\n\n\n\n5. Outliers: Z-scores\n\ndef zscoring(df, columns, threshold =3):\n    z_scores_dict = {}\n    for col in columns:\n        z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n        z_scores_dict[col + '_zscore'] = z_scores\n    z_scores_df = pd.DataFrame(z_scores_dict)\n    return z_scores_df\n\nz_scores_sdf = zscoring(sdf, intCol)\nz_scores_sdf\n\n\n\n\n\n\n\n\nAll Time Rank_zscore\nTrack Score_zscore\nSpotify Streams_zscore\nSpotify Playlist Count_zscore\nSpotify Playlist Reach_zscore\nSpotify Popularity_zscore\nYouTube Views_zscore\nYouTube Likes_zscore\nTikTok Posts_zscore\nTikTok Likes_zscore\n...\nYouTube Playlist Reach_zscore\nApple Music Playlist Count_zscore\nAirPlay Spins_zscore\nDeezer Playlist Count_zscore\nDeezer Playlist Reach_zscore\nAmazon Playlist Count_zscore\nPandora Streams_zscore\nPandora Track Stations_zscore\nShazam Counts_zscore\nExplicit Track_zscore\n\n\n\n\n0\n1.730622\n17.732756\n0.107108\n0.406289\n5.882628\n1.938203\n0.469887\n0.274599\n2.288098\n1.104820\n...\n0.326421\n2.315517\n0.117118\n0.611515\n5.131862\n3.885922\n0.462468\n0.292580\n0.024738\n0.748277\n\n\n1\n1.729866\n13.076162\n0.232649\n0.443162\n5.134597\n1.938203\n0.422586\n0.125419\n0.127990\n0.158752\n...\n0.316659\n1.987692\n0.118749\n0.714522\n2.872978\n3.754428\n0.532345\n0.267749\n0.249878\n1.336112\n\n\n2\n1.729111\n12.881597\n0.289327\n0.071771\n6.391039\n1.938203\n0.413365\n0.158310\n0.987168\n0.333133\n...\n0.050303\n2.017495\n0.159020\n2.136021\n11.025354\n6.428142\n0.551190\n0.370464\n0.487939\n0.748277\n\n\n3\n1.728355\n10.456017\n2.978073\n2.980478\n3.843613\n1.462161\n1.022392\n1.736456\n2.962738\n1.980611\n...\n5.075927\n5.057322\n11.752106\n4.773004\n7.362179\n8.093734\n0.714773\n0.520190\n1.645482\n0.748277\n\n\n4\n1.727599\n9.895669\n0.640047\n0.739078\n4.349468\n1.666179\n0.480065\n0.166794\n0.440283\n0.000044\n...\n0.390280\n1.898286\n0.355442\n1.023543\n5.151349\n3.491440\n0.554804\n0.364307\n0.366960\n1.336112\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4594\n1.734176\n0.582317\n0.267724\n0.090268\n0.619423\n0.510078\n0.374063\n0.345765\n0.425117\n0.189547\n...\n0.580527\n0.769014\n0.456260\n0.624571\n0.403222\n0.000225\n0.448120\n0.336481\n0.331669\n1.336112\n\n\n4595\n1.726618\n0.582317\n0.742998\n0.776206\n0.743446\n0.510012\n0.797950\n0.545153\n0.000089\n0.230960\n...\n0.538464\n0.798816\n0.452899\n0.645173\n0.407405\n0.000225\n0.000043\n0.000060\n0.413602\n0.748277\n\n\n4596\n1.723594\n0.582317\n0.484101\n0.179454\n0.565047\n0.102042\n0.297696\n0.417077\n0.444460\n0.220121\n...\n0.531240\n0.530596\n0.454621\n0.645173\n0.407674\n0.847865\n0.008521\n0.265249\n0.246740\n1.336112\n\n\n4597\n1.738711\n0.582317\n0.567784\n0.642555\n0.541885\n0.170048\n1.055567\n0.211595\n0.442639\n0.225767\n...\n0.538164\n0.798816\n0.446376\n0.000122\n0.000089\n0.804034\n0.538921\n0.000060\n0.368505\n0.748277\n\n\n4598\n1.740223\n0.582317\n0.360439\n0.386146\n0.315129\n0.034036\n0.683474\n0.202123\n0.411060\n0.210349\n...\n0.283827\n0.649805\n0.439067\n0.583369\n0.367571\n0.935528\n0.113906\n0.344876\n0.312074\n1.336112\n\n\n\n\n4599 rows × 21 columns\n\n\n\n\ndef remove_outliers(df, z_scores_sdf, threshold=3):\n    # Identify rows with any Z-score greater than threshold\n    outliers_mask = (z_scores_sdf &gt; threshold).any(axis=1)\n    \n    # Remove outliers from main DataFrame\n    df_clean = df[~outliers_mask]\n    \n    return df_clean\n\nsdfClean = remove_outliers(sdf, z_scores_sdf)\n\n\nsdf = sdfClean.reset_index()\nsdf\n\n\n\n\n\n\n\n\nlevel_0\nindex\nTrack\nAlbum Name\nArtist\nRelease Date\nISRC\nAll Time Rank\nTrack Score\nSpotify Streams\n...\nYouTube Playlist Reach\nApple Music Playlist Count\nAirPlay Spins\nDeezer Playlist Count\nDeezer Playlist Reach\nAmazon Playlist Count\nPandora Streams\nPandora Track Stations\nShazam Counts\nExplicit Track\n\n\n\n\n0\n98\n98\nAgora Hills\nScarlet\nDoja Cat\n9/22/2023\nUSRC12301954\n99\n152.8\n509653100.0\n...\n1.561457e+09\n80.0\n400206.000000\n78.000000\n9.341155e+06\n39.000000\n2.311027e+07\n19559.000000\n4295361.0\n1\n\n\n1\n100\n100\nMontagem Rave Eterno\nMontagem Rave Eterno\nDj Samir\n5/2/2024\nGXD7G2413058\n101\n152.3\n5157486.0\n...\n3.439601e+08\n3.0\n55139.156753\n2.000000\n3.936600e+04\n25.348942\n8.566735e+07\n87876.965786\n47658.0\n0\n\n\n2\n101\n101\nLil Boo Thang\nLil Boo Thang\nPaul Russell\n8/18/2023\nUSAR12300323\n102\n152.1\n242070373.0\n...\n1.559373e+07\n106.0\n370635.000000\n69.000000\n1.792014e+06\n79.000000\n8.442572e+07\n30713.000000\n2219984.0\n0\n\n\n3\n106\n106\nType Shit\nWE DON'T TRUST YOU\nFuture\n3/22/2024\nUSSM12402033\n107\n148.1\n205206688.0\n...\n2.324409e+08\n99.0\n3432.000000\n41.000000\n2.943724e+06\n47.000000\n5.897303e+06\n4527.000000\n900933.0\n1\n\n\n4\n107\n107\nArmageddon\nArmageddon - The 1st Album\naespa\n5/27/2024\nKRA302400093\n108\n147.4\n32942304.0\n...\n2.951077e+07\n37.0\n290.000000\n5.000000\n5.308310e+05\n33.000000\n2.186900e+04\n48.000000\n29736.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3689\n4594\n4595\nFor the Last Time\nFor the Last Time\n$uicideboy$\n9/5/2017\nQM8DG1703420\n4585\n19.4\n305049963.0\n...\n5.301600e+04\n3.0\n6.000000\n2.000000\n1.421700e+04\n25.348942\n2.010407e+07\n13184.000000\n656337.0\n1\n\n\n3690\n4595\n4596\nDil Meri Na Sune\nDil Meri Na Sune (From \"Genius\")\nAtif Aslam\n7/27/2018\nINT101800122\n4575\n19.4\n52282360.0\n...\n2.497305e+07\n1.0\n412.000000\n1.000000\n9.270000e+02\n25.348942\n8.566735e+07\n87876.965786\n193590.0\n0\n\n\n3691\n4596\n4597\nGrace (feat. 42 Dugg)\nMy Turn\nLil Baby\n2/28/2020\nUSUG12000043\n4571\n19.4\n189972685.0\n...\n2.925315e+07\n19.0\n204.000000\n1.000000\n7.400000e+01\n6.000000\n8.442674e+07\n28999.000000\n1135998.0\n1\n\n\n3692\n4597\n4598\nNashe Si Chadh Gayi\nNovember Top 10 Songs\nArijit Singh\n11/8/2016\nINY091600067\n4591\n19.4\n145467020.0\n...\n2.515052e+07\n1.0\n1200.000000\n32.310954\n1.294939e+06\n7.000000\n6.817840e+06\n87876.965786\n448292.0\n0\n\n\n3693\n4598\n4599\nMe Acostumbre (feat. Bad Bunny)\nMe Acostumbre (feat. Bad Bunny)\nArcï¿½ï¿½\n4/11/2017\nUSB271700107\n4593\n19.4\n255740653.0\n...\n1.758314e+08\n11.0\n2083.000000\n4.000000\n1.274790e+05\n4.000000\n6.900674e+07\n11320.000000\n767006.0\n1\n\n\n\n\n3694 rows × 28 columns\n\n\n\n\n\n6. Formating Date\n\nsdf['Release Date'] = pd.to_datetime(sdf['Release Date'])\n\n\nsdf['Release Month'] = sdf['Release Date'].dt.month\nsdf['Release Year'] = sdf['Release Date'].dt.year\nsdf\n\n\n\n\n\n\n\n\nlevel_0\nindex\nTrack\nAlbum Name\nArtist\nRelease Date\nISRC\nAll Time Rank\nTrack Score\nSpotify Streams\n...\nAirPlay Spins\nDeezer Playlist Count\nDeezer Playlist Reach\nAmazon Playlist Count\nPandora Streams\nPandora Track Stations\nShazam Counts\nExplicit Track\nRelease Month\nRelease Year\n\n\n\n\n0\n98\n98\nAgora Hills\nScarlet\nDoja Cat\n2023-09-22\nUSRC12301954\n99\n152.8\n509653100.0\n...\n400206.000000\n78.000000\n9.341155e+06\n39.000000\n2.311027e+07\n19559.000000\n4295361.0\n1\n9\n2023\n\n\n1\n100\n100\nMontagem Rave Eterno\nMontagem Rave Eterno\nDj Samir\n2024-05-02\nGXD7G2413058\n101\n152.3\n5157486.0\n...\n55139.156753\n2.000000\n3.936600e+04\n25.348942\n8.566735e+07\n87876.965786\n47658.0\n0\n5\n2024\n\n\n2\n101\n101\nLil Boo Thang\nLil Boo Thang\nPaul Russell\n2023-08-18\nUSAR12300323\n102\n152.1\n242070373.0\n...\n370635.000000\n69.000000\n1.792014e+06\n79.000000\n8.442572e+07\n30713.000000\n2219984.0\n0\n8\n2023\n\n\n3\n106\n106\nType Shit\nWE DON'T TRUST YOU\nFuture\n2024-03-22\nUSSM12402033\n107\n148.1\n205206688.0\n...\n3432.000000\n41.000000\n2.943724e+06\n47.000000\n5.897303e+06\n4527.000000\n900933.0\n1\n3\n2024\n\n\n4\n107\n107\nArmageddon\nArmageddon - The 1st Album\naespa\n2024-05-27\nKRA302400093\n108\n147.4\n32942304.0\n...\n290.000000\n5.000000\n5.308310e+05\n33.000000\n2.186900e+04\n48.000000\n29736.0\n0\n5\n2024\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3689\n4594\n4595\nFor the Last Time\nFor the Last Time\n$uicideboy$\n2017-09-05\nQM8DG1703420\n4585\n19.4\n305049963.0\n...\n6.000000\n2.000000\n1.421700e+04\n25.348942\n2.010407e+07\n13184.000000\n656337.0\n1\n9\n2017\n\n\n3690\n4595\n4596\nDil Meri Na Sune\nDil Meri Na Sune (From \"Genius\")\nAtif Aslam\n2018-07-27\nINT101800122\n4575\n19.4\n52282360.0\n...\n412.000000\n1.000000\n9.270000e+02\n25.348942\n8.566735e+07\n87876.965786\n193590.0\n0\n7\n2018\n\n\n3691\n4596\n4597\nGrace (feat. 42 Dugg)\nMy Turn\nLil Baby\n2020-02-28\nUSUG12000043\n4571\n19.4\n189972685.0\n...\n204.000000\n1.000000\n7.400000e+01\n6.000000\n8.442674e+07\n28999.000000\n1135998.0\n1\n2\n2020\n\n\n3692\n4597\n4598\nNashe Si Chadh Gayi\nNovember Top 10 Songs\nArijit Singh\n2016-11-08\nINY091600067\n4591\n19.4\n145467020.0\n...\n1200.000000\n32.310954\n1.294939e+06\n7.000000\n6.817840e+06\n87876.965786\n448292.0\n0\n11\n2016\n\n\n3693\n4598\n4599\nMe Acostumbre (feat. Bad Bunny)\nMe Acostumbre (feat. Bad Bunny)\nArcï¿½ï¿½\n2017-04-11\nUSB271700107\n4593\n19.4\n255740653.0\n...\n2083.000000\n4.000000\n1.274790e+05\n4.000000\n6.900674e+07\n11320.000000\n767006.0\n1\n4\n2017\n\n\n\n\n3694 rows × 30 columns"
  },
  {
    "objectID": "projects/Desirability_Maxxing.html",
    "href": "projects/Desirability_Maxxing.html",
    "title": "Optimizing Desirability",
    "section": "",
    "text": "Optimizing Desirability\n\n\nPreface\nPart two of an ongoing project where I analyze data surrounding my part time job as a pizza delivery driver. Part two focuses on geographic data. It was originally submitted as an assignment for GEOG 255 at Simon Fraser University.\n\n\nSection I. Research Question and Theory\nBased on the success the first part of this project had, I decided to extend the scope of this project to include geographic questions. The research circumstances being studied are explained in part one. An important bit of exposition required is that while Me-n-Ed’s is located in Port Coquitlam, it isn’t limited to delivering to that municipality. Additionally, we deliver to 3 neighbouring municipalities for a total of 4: Coquitlam, Port Coquitlam, Port Moody and Anmore. An important part of making sure I’m getting the most tip money on a given shift is to not waste time on the road; the faster I get back to the restaurant, the more deliveries I take and the more tip money I make. Deliveries to closer addresses are preferable for this reason; less time spent in transit. The ideal delivery will therefore be high-tipping, and nearby. One could represent the desirability of a delivery as a ratio of its tip over it’s distance from the restaurant. If that’s the case, my question is which municipality will be on average the most desirable to deliver to? To answer this question, I intend on producing a map visualizing the shortest routes[^1] to a set of delivery addresses. This involves network analysis based on the location of Me-n-Ed’s and the local road network. Furthermore, I will generate a choropleth layer representing the desirability of each municipality.\nFurthermore, I have an additional speculative inquiry. In the past month or so, management has been gossiping about the possibility of opening another Me-n-Ed’s right in the center of Port Moody. In this hypothetical scenario, deliveries closest to the Port Moody location will be taken by the Port Moody location, instead of the Port Coquitlam location. Because Port Moody is pretty close to where I live, I could ask to be transferred to the Port Moody branch and work there instead if I wanted to. In the interest of maximizing my income from this job, which location would be the most desirable to work at? To see if switching to the speculative Port Moody location would be a good idea, I’ll recalculate the shortest distance from each delivery in my dataset, this time to the closest Me-n-Ed’s. This will require me to recalculate the desirability of each delivery. By comparing the respective average desirability of both locations’ deliveries, I’ll be able to determine which will be the most desirable. # Section II. Datasets and Data Acquisition My main dataset is the same one I gathered and used in part one. My data includes Date, Tip Amount, Address, Municipality, Desirability, Location of the restaurant in Port Coquitlam and the Potential location of the restaurant in Port Moody. The potential location of the Port Moody location is speculatively assumed to be in downtown Port Moody, which is not unrealistic. Desirability was calculated via a simple SQL expression: \\((tip/distance)*100\\). Road network data and municipality boundaries was accessed via BC Data Catalogue (CRF = NAD83). Road network data was trimmed to fit area of interest. # Section III. Shortest Manhattan distances and Desirability\n Based on my analysis, I’ve generated a choropleth map visualizing the shortest distance from each delivery to Port Coquitlam location and the average desirability of each municipality. We can see that the most desirable municipalities are Port Coquitlam, Coquitlam, Port Moody, and Anmore in that order.\n When we take the average desirability of every delivery taken by either respective location, we can see that Port Moody &gt; Port Coquitlam. According to this summary, I’ve produced a map showing the shortest route from each restaurant to the nearest Me-n-Ed’s.\n\n\nSection IV. Conclusion\nI can conclude from my research that Port Coquitlam is the most desirable municipality to deliver to when working at the Port Coquitlam location. However, overall, the hypothetical Porty Moody location is the more desirable of the two locations.\n  [^1]: Luckily, the fastest route will almost always also be the shortest route, because there are no one-ways, substantial shortcuts and only one highway. This makes things much simpler. Of course construction, road closures, vehicle collisions and traffic might make the expected routes take longer, or simply make them impossible to take, but these are rare and unpredictable enough to deal with on a case by case basis."
  },
  {
    "objectID": "projects/Melee-SQL.html",
    "href": "projects/Melee-SQL.html",
    "title": "Super Smash Brothers Player Statistics",
    "section": "",
    "text": "This is a SQL project focused on measuring and improving my performance in the fighting game Super Smash Brothers Melee.\nQueries used:\nWITH... AS (AKA CTEs)\nWHERE... (AND...)\nHAVING\nROUND\nSUM\nCOUNT\nGROUP BY\nORDER BY\nCASE... (WHEN... THEN... END)\nCAST\nLEFT JOIN\nSTRFTIME\nDATE\nTIME\nBETWEEN\n\n\n\n\n\nSQL Project: Super Smash Bros. Melee\n\nSummary\nTable of Contents\nSection I. Research Questions\nSection II. Data Collection\nSection III. SQL Queries\n\n1. How many games have I played using “Falco”?\n2. What are my best and worst matchups?\n3. What are my best and worst stages?\n4. What are my best and worst matchups on each stage?\n5. When do I play best?\n\n5.1. Day of the Week\n5.2. Time of Day\n5.3. Day of the Week and Time of Day\n\n\nSection IV. Interpretation and Key Insights\n\nKey Insights\nMatchups\nStages\nDay of Week/Time of Day\n\n\n\n\n\n\nSuper Smash Bros. Melee (hereon, “Melee”) is a fighting game for the Nintendo GameCube, which was released on November 21st, 2001. More than two decades later, people are still playing Melee. More specifically, the game’s competitive community is thriving despite the game’s age. How could such an old game still have such an active fan base? Slippi. Slippi is an ongoing modding project by members of the Melee community which aims to render several major quality-of-life additions to the game. The two most major changes were rollback netcode and an integrated online matchmaking system. I started playing Melee in 2023 after hearing about this modding project and have been playing since.\nIn addition to these two features, another interesting addition the Slippi team made was an automatic replay saving feature, which saves a replay of every match of Melee you play online. The replay files are stored right onto the device running Slippi as a .slp file, which can be viewed through the Slippi launcher. What’s interesting about these files are that they are laden with data. By passing the replay files through a parsing program, we can view all sorts of data: Player characters and names, the winner of each game, the date and time of each game, the stage played on…\nThe goal of this project is to kill two birds with one stone: get better at SQL and Melee. By parsing my replay files, and placing them into a database, I can practice writing SQL scripts that pull key insights about my gameplay out of my replay database. Once I have these key insights, I can determine my strengths and weaknesses and adapt my playstyle accordingly in-game.\nIt’s also important to mention that there are many different characters to play in Melee, but most players pick one character and stick to them. In my case, I’m interested in playing Falco.\nGiven that, these are the questions I wanted answered:\n\n\nHow many games have I played on Falco?\n\nCount of games on Falco.\n\nWhat are my best and worst matchups?\n\nAverage win-rate vs every character.\n\nWhat are my best and worst stages?\n\nAverage win-rate on every stage.\n\nWhat are my best and worst matchups on each stage?\n\nAverage win-rates on every matchup-stage combination.\n\nWhen do I play best?\n\nAverage win-rate on every day of the week.\nAverage win-rate at different times of day (e.g. Morning)\nAverage win-rate on every day of the week and at every time of day.\n\n\n\n\n\n\nI used Slippi DB to parse replay files and place them into a SQLite database.\nI’ve been accumulating replays over the course of approximately a year (since I started playing in 2023).\n\nTotal number of replays: 3313\n\n\n\n\n\n\n-- Starting by filtering down to only the games where I play Falco\nWITH\nfalco_games AS (\n    select\n        me.character as bird \n    from players me, players op \n    where me.id != op.id -- x /= y (there are two unique objects in the domain)\n    and me.game_id = op.game_id -- Gxy & Gyx (One object is in the same game as the other, and vice versa)\n    and me.character='FALCO' -- Cx'Falco'  (One object has the property of playing Falco)\n    and me.code='FROG#671' -- Ox'FROG#671' (That object also has my specific code)\n) \nselect bird as Character, count(*) as \"Games played\" -- Counting Games\nFROM falco_games;\nOutput:\n\n\n\nCharacter\nGames played\n\n\n\n\nFALCO\n2332\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id, \n        me.game_id, \n        op.character as matchups, \n        me.winner as didIwin\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nselect matchups as 'Matchups',\n    round((sum(didIwin)*1.0 /count(*)*100), 1) as Winrate, \n    -- Winrate: The percetentage of time I win...\n    count(*) as 'Games played'\nFROM falco_games\nGROUP BY matchups                                           -- ... each matchup\nORDER BY Winrate desc;\nOutput, ordered by Winrate:\n\n\n\nMatchups\nWinrate\nGames played\n\n\n\n\nMEWTWO\n66.7\n9\n\n\nGAME_AND_WATCH\n66.7\n12\n\n\nBOWSER\n66.7\n3\n\n\nYOUNG_LINK\n63.6\n11\n\n\nROY\n60\n15\n\n\nPIKACHU\n57.1\n14\n\n\nSAMUS\n56.7\n30\n\n\nCAPTAIN_FALCON\n54.7\n338\n\n\nMARIO\n52.9\n34\n\n\nDONKEY_KONG\n52.9\n51\n\n\nSHEIK\n52.8\n159\n\n\nFOX\n51.3\n392\n\n\nFALCO\n48.7\n622\n\n\nYOSHI\n46.2\n13\n\n\nNESS\n45.5\n11\n\n\nLUIGI\n45.5\n44\n\n\nMARTH\n44.9\n365\n\n\nKIRBY\n42.9\n7\n\n\nZELDA\n40\n5\n\n\nGANONDORF\n34.2\n76\n\n\nLINK\n33.3\n21\n\n\nICE_CLIMBERS\n33.3\n6\n\n\nPEACH\n31.3\n48\n\n\nJIGGLYPUFF\n27.3\n33\n\n\nDR_MARIO\n25\n12\n\n\nPICHU\n0\n1\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id, \n        me.game_id as gid, \n        me.winner as winner\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nselect stage as Stage,\n   round((sum(winner)*1.0/count(*)*100),1) as Winrate,\n    count(*)\nFrom falco_games\nLEFT JOIN games on gid = games.id --Left joining with games to get stage data\nGROUP BY Stage\nHAVING count(*) &gt; 10 -- Ignore games on stages I've played less than 10 times\nORDER BY Winrate desc;\nOutput, ordered by Winrate:\n\n\n\nStage\nWinrate\ncount(*)\n\n\n\n\nFINAL_DESTINATION\n52.6\n382\n\n\nPOKEMON_STADIUM\n52.4\n393\n\n\nFOUNTAIN_OF_DREAMS\n47.7\n392\n\n\nYOSHIS_STORY\n46.8\n389\n\n\nDREAM_LAND_N64\n46.6\n371\n\n\nBATTLEFIELD\n46.1\n399\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id as mid, \n        me.game_id as gid,  \n        op.character as matchup, \n        me.winner as winner\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nSelect stage as Stage, matchup,\n    round((sum(winner)*1.0/count(*)*100), 1) as Winrate,\n    count(*)\nFrom falco_games\nLEFT JOIN games on gid = games.id\nGROUP BY Stage, matchup\nHAVING count(*) &gt;= 10 -- filtering out stage-matchup combinations that I have played on/against less than 10 times\nORDER BY Winrate desc;\nOutput, ordered by Winrate:\n\n\n\nStage\nmatchup\nWinrate\ncount(*)\n\n\n\n\nYOSHIS_STORY\nDONKEY_KONG\n70\n10\n\n\nFINAL_DESTINATION\nSHEIK\n62.5\n32\n\n\nBATTLEFIELD\nCAPTAIN_FALCON\n61.3\n62\n\n\nPOKEMON_STADIUM\nFOX\n61.3\n62\n\n\nFOUNTAIN_OF_DREAMS\nCAPTAIN_FALCON\n60.3\n58\n\n\nPOKEMON_STADIUM\nLUIGI\n60\n10\n\n\nBATTLEFIELD\nSHEIK\n57.1\n28\n\n\nFOUNTAIN_OF_DREAMS\nSHEIK\n57.1\n21\n\n\nFINAL_DESTINATION\nMARTH\n56.3\n64\n\n\nFINAL_DESTINATION\nFOX\n55.1\n69\n\n\nYOSHIS_STORY\nCAPTAIN_FALCON\n54.4\n57\n\n\nFINAL_DESTINATION\nFALCO\n53.8\n93\n\n\nDREAM_LAND_N64\nCAPTAIN_FALCON\n53.7\n54\n\n\nPOKEMON_STADIUM\nFALCO\n53.7\n108\n\n\nPOKEMON_STADIUM\nSHEIK\n51.9\n27\n\n\nFINAL_DESTINATION\nGANONDORF\n50\n14\n\n\nFOUNTAIN_OF_DREAMS\nMARTH\n50\n54\n\n\nPOKEMON_STADIUM\nCAPTAIN_FALCON\n50\n52\n\n\nYOSHIS_STORY\nFOX\n48.5\n66\n\n\nDREAM_LAND_N64\nFOX\n48.1\n54\n\n\nDREAM_LAND_N64\nFALCO\n47.4\n97\n\n\nFINAL_DESTINATION\nCAPTAIN_FALCON\n47.3\n55\n\n\nBATTLEFIELD\nFALCO\n47.1\n102\n\n\nFOUNTAIN_OF_DREAMS\nFOX\n47\n66\n\n\nBATTLEFIELD\nFOX\n46.6\n73\n\n\nYOSHIS_STORY\nFALCO\n45.7\n105\n\n\nBATTLEFIELD\nPEACH\n45.5\n11\n\n\nFOUNTAIN_OF_DREAMS\nFALCO\n45.3\n117\n\n\nDREAM_LAND_N64\nSHEIK\n44.8\n29\n\n\nDREAM_LAND_N64\nGANONDORF\n43.8\n16\n\n\nPOKEMON_STADIUM\nMARTH\n43.5\n69\n\n\nYOSHIS_STORY\nMARTH\n41.7\n60\n\n\nYOSHIS_STORY\nSHEIK\n40.9\n22\n\n\nDREAM_LAND_N64\nMARTH\n39.7\n58\n\n\nBATTLEFIELD\nMARTH\n39\n59\n\n\nFOUNTAIN_OF_DREAMS\nGANONDORF\n36.4\n11\n\n\nDREAM_LAND_N64\nDONKEY_KONG\n30\n10\n\n\nYOSHIS_STORY\nGANONDORF\n28.6\n14\n\n\nPOKEMON_STADIUM\nGANONDORF\n27.3\n11\n\n\nBATTLEFIELD\nGANONDORF\n10\n10\n\n\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id as mid, \n        me.game_id as gid, \n        me.winner as winner\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nselect\nCASE CAST (strftime('%w', DATE(start_time)) as int)   --strftime(%w,...) to get the day of the week based on the date of the game\n    WHEN 0 then 'Sunday'-- Instead of expressing each day of week as an integer (sunday=0... saturday=6), strings\n    WHEN 1 then 'Monday'\n    WHEN 2 then 'Tuesday'\n    WHEN 3 then 'Wednesday'\n    WHEN 4 then 'Thursday'\n    WHEN 5 then 'Friday'\n    else 'Saturday' end as \"Day of Week\",\n    round((sum(winner)*1.0/count(*)*100), 1) as Winrate,\n    count(*) as Games\nFrom falco_games\nLEFT JOIN games on gid = games.id\nGROUP BY 1\nORDER BY Winrate desc;\nOutput, ordered by Winrate:\n\n\n\nDay of Week\nWinrate\nGames\n\n\n\n\nWednesday\n52.3\n369\n\n\nThursday\n50.9\n379\n\n\nSaturday\n48.1\n318\n\n\nTuesday\n47.5\n396\n\n\nMonday\n47.4\n388\n\n\nFriday\n47.2\n299\n\n\nSunday\n46.4\n183\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id as mid, \n        me.game_id as gid, \n        me.winner as winner\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nselect \n    CASE -- Defining different times of the day based on the hour of the day\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 0 AND 5 THEN 'Late Night (12AM-5AM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 5 AND 9 THEN 'Morning (5AM-9AM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 9 AND 14 THEN 'Noon (9AM-2PM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 14 AND 19 THEN 'Afternoon (2PM-7PM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) &gt;= 19 THEN 'Evening (7PM-12AM)'\n    END AS \"Time of Day\",\n    round((sum(winner)*1.0/count(*)*100), 1) as Winrate,\n    count(*) as \"Games Played\"\nFrom falco_games\nLEFT JOIN games on gid = games.id\nGROUP BY 1\nORDER BY CASE -- Aesthetic reordering so 'Morning' comes first, and 'Late Night' comes last\n    WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 0 AND 5 THEN 5\n    WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 5 AND 9 THEN 1\n    WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 9 AND 14 THEN 2\n    WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 14 AND 19 THEN 3\n    WHEN CAST (strftime('%H', TIME(start_time)) as int) &gt;= 19 THEN 4\nEND;\nOutput, ordered by Winrate:\n\n\n\nTime of Day\nWinrate\nGames Played\n\n\n\n\nMorning (5AM-9AM)\n47.8\n370\n\n\nNoon (9AM-2PM)\n61.5\n13\n\n\nAfternoon (2PM-7PM)\n48.6\n210\n\n\nEvening (7PM-12AM)\n46.2\n650\n\n\nLate Night (12AM-5AM)\n50.5\n1089\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id as mid, \n        me.game_id as gid, \n        me.winner as winner\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nselect\n    CASE CAST (strftime('%w', DATE(start_time)) as int) \n        WHEN 0 then 'Sunday'\n        WHEN 1 then 'Monday'\n        WHEN 2  then 'Tuesday'\n        WHEN 3 then 'Wednesday'\n        WHEN 4 then 'Thursday'\n        WHEN 5 then 'Friday'\n        else 'Saturday' \n    END as 'Day of Week',\n    CASE \n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 0 AND 5 THEN 'Late Night (12AM-5AM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 5 AND 9 THEN 'Morning (5AM-9AM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 9 AND 14 THEN 'Noon (9AM-2PM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 14 AND 19 THEN 'Afternoon (2PM-7PM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) &gt;= 19 THEN 'Evening (7PM-12AM)'\n    END AS 'Time of Day',\n    round((sum(winner)*1.0/count(*)*100), 1) as Winrate,\n    count(*) as Games\nFrom falco_games\nLEFT JOIN games on gid = games.id\nGROUP BY 1, 2\nHAVING Games &gt;= 10\nORDER BY Winrate desc;\nOutput, ordered by Winrate:\n\n\n\nDay of Week\nTime of Day\nWinrate\nGames\n\n\n\n\nThursday\nLate Night (12AM-5AM)\n56\n184\n\n\nWednesday\nAfternoon (2PM-7PM)\n56\n75\n\n\nWednesday\nEvening (7PM-12AM)\n55.1\n78\n\n\nMonday\nMorning (5AM-9AM)\n54.5\n33\n\n\nSunday\nAfternoon (2PM-7PM)\n54.5\n11\n\n\nTuesday\nMorning (5AM-9AM)\n53.3\n15\n\n\nTuesday\nAfternoon (2PM-7PM)\n53.1\n32\n\n\nSaturday\nLate Night (12AM-5AM)\n52.4\n187\n\n\nWednesday\nLate Night (12AM-5AM)\n52.4\n126\n\n\nMonday\nLate Night (12AM-5AM)\n50\n184\n\n\nSaturday\nMorning (5AM-9AM)\n50\n32\n\n\nWednesday\nNoon (9AM-2PM)\n50\n10\n\n\nSunday\nMorning (5AM-9AM)\n49.4\n89\n\n\nFriday\nMorning (5AM-9AM)\n48.8\n43\n\n\nFriday\nAfternoon (2PM-7PM)\n47.5\n40\n\n\nTuesday\nLate Night (12AM-5AM)\n47.1\n261\n\n\nFriday\nLate Night (12AM-5AM)\n46.9\n98\n\n\nFriday\nEvening (7PM-12AM)\n46.6\n118\n\n\nWednesday\nMorning (5AM-9AM)\n46.3\n80\n\n\nThursday\nEvening (7PM-12AM)\n46.2\n106\n\n\nTuesday\nEvening (7PM-12AM)\n45.5\n88\n\n\nSunday\nLate Night (12AM-5AM)\n44.9\n49\n\n\nSaturday\nEvening (7PM-12AM)\n44.7\n76\n\n\nMonday\nEvening (7PM-12AM)\n44\n150\n\n\nThursday\nMorning (5AM-9AM)\n42.3\n78\n\n\nSunday\nEvening (7PM-12AM)\n38.2\n34\n\n\nMonday\nAfternoon (2PM-7PM)\n38.1\n21\n\n\nSaturday\nAfternoon (2PM-7PM)\n21.7\n23\n\n\n\n\n\n\n\n\n\n\n\n\nI need practice against Jigglypuff.\nI need practice on Battlefield.\nCounterpick to Final Destination against Sheik, Marth and Fox.\nAvoid Battlefield against Marth, Peach, Fox and Falco.\nAgainst Captain Falcon, counterpick to Battlefield and avoid Final Destination. ### Matchups First, looking at my matchup win-rates, we can see that my best matchups are against Mewtwo, Bowser, and Game and Watch. This makes sense because these characters are generally considered weak characters according to the community tier-list (i.e., a “low-tier” character), which also explains why I’ve played against so few of them. Compare this to someone like Captain Falcon, who I’ve played against many more times (n=338). My matchups against high-tiers generally matter more than against non-high-tiers, simply because more people play them.\n\n\nMy highest win-rate against a “high-tier” character is 54.7% against Captain Falcon, and my lowest is 27.3% against Jigglypuff. This suggests that I need more practice against Jigglypuff, and that I’m well practiced against Captain Falcon.\n\n\n\nLooking at my stage win-rates, it’s interesting to see that Final Destination is my best stage. Traditional wisdom suggests Falco, my character, is bad on Final Destination. This is bizarre, but situationally advantageous when counter-picking a stage in tournament. 1 Looking at my stage-matchup win-rates, I am more than 50% likely to beat Sheik, Marth and Fox on Final Destination, who all generally like to counterpick to Final Destination. However, my win-rate against Captain Falcon is losing on Final Destination at 47.3%. So, if I beat Sheik, Marth or Fox in game 1, they might unknowingly counterpick my best stage. If I lose in game 1, I know to counterpick to Final Destination, unless I’m playing against Captain Falcon.\nFurthermore, Battlefield is my worst stage, which according to traditional wisdom should be one of Falco’s best stages. Looking at my stage-matchup win-rates, I can see that I am more likely to lose on Battlefield against Marth, Peach, Fox, and Falco. So, I should avoid Battlefield against Marth, Peach, Fox and Falco whenever possible. But funnily enough, Captain Falcon is again the exception. My win-rate against Captain Falcon on Battlefield is my third best mathcup at 61.3%. So, I should pick Battlefield against Captain Falcon.\n\n\n\nI was very surprised to see that I play the best on Wednesdays. This was also reflected in the Day of the Week - Time of Day win-rates, where my top 3 performances were on varying times on Wednesday. 2 I can only speculate, but I suspect this outcome is a result of my and other people’s work schedules. For around a year Wednesdays were my day off, which gave me more time to relax and play Melee. Generally, people don’t have Wednesdays off, so my opponents were probably suffering from the stress of work on Wednesdays. I believe it was a combination of my atypical and other people’s typical work schedule. This simply suggests that stress makes people perform worse in Melee."
  },
  {
    "objectID": "projects/Melee-SQL.html#summary",
    "href": "projects/Melee-SQL.html#summary",
    "title": "Super Smash Brothers Player Statistics",
    "section": "",
    "text": "This is a SQL project focused on measuring and improving my performance in the fighting game Super Smash Brothers Melee.\nQueries used:\nWITH... AS (AKA CTEs)\nWHERE... (AND...)\nHAVING\nROUND\nSUM\nCOUNT\nGROUP BY\nORDER BY\nCASE... (WHEN... THEN... END)\nCAST\nLEFT JOIN\nSTRFTIME\nDATE\nTIME\nBETWEEN"
  },
  {
    "objectID": "projects/Melee-SQL.html#table-of-contents",
    "href": "projects/Melee-SQL.html#table-of-contents",
    "title": "Super Smash Brothers Player Statistics",
    "section": "",
    "text": "SQL Project: Super Smash Bros. Melee\n\nSummary\nTable of Contents\nSection I. Research Questions\nSection II. Data Collection\nSection III. SQL Queries\n\n1. How many games have I played using “Falco”?\n2. What are my best and worst matchups?\n3. What are my best and worst stages?\n4. What are my best and worst matchups on each stage?\n5. When do I play best?\n\n5.1. Day of the Week\n5.2. Time of Day\n5.3. Day of the Week and Time of Day\n\n\nSection IV. Interpretation and Key Insights\n\nKey Insights\nMatchups\nStages\nDay of Week/Time of Day"
  },
  {
    "objectID": "projects/Melee-SQL.html#section-i.-research-questions",
    "href": "projects/Melee-SQL.html#section-i.-research-questions",
    "title": "Super Smash Brothers Player Statistics",
    "section": "",
    "text": "Super Smash Bros. Melee (hereon, “Melee”) is a fighting game for the Nintendo GameCube, which was released on November 21st, 2001. More than two decades later, people are still playing Melee. More specifically, the game’s competitive community is thriving despite the game’s age. How could such an old game still have such an active fan base? Slippi. Slippi is an ongoing modding project by members of the Melee community which aims to render several major quality-of-life additions to the game. The two most major changes were rollback netcode and an integrated online matchmaking system. I started playing Melee in 2023 after hearing about this modding project and have been playing since.\nIn addition to these two features, another interesting addition the Slippi team made was an automatic replay saving feature, which saves a replay of every match of Melee you play online. The replay files are stored right onto the device running Slippi as a .slp file, which can be viewed through the Slippi launcher. What’s interesting about these files are that they are laden with data. By passing the replay files through a parsing program, we can view all sorts of data: Player characters and names, the winner of each game, the date and time of each game, the stage played on…\nThe goal of this project is to kill two birds with one stone: get better at SQL and Melee. By parsing my replay files, and placing them into a database, I can practice writing SQL scripts that pull key insights about my gameplay out of my replay database. Once I have these key insights, I can determine my strengths and weaknesses and adapt my playstyle accordingly in-game.\nIt’s also important to mention that there are many different characters to play in Melee, but most players pick one character and stick to them. In my case, I’m interested in playing Falco.\nGiven that, these are the questions I wanted answered:\n\n\nHow many games have I played on Falco?\n\nCount of games on Falco.\n\nWhat are my best and worst matchups?\n\nAverage win-rate vs every character.\n\nWhat are my best and worst stages?\n\nAverage win-rate on every stage.\n\nWhat are my best and worst matchups on each stage?\n\nAverage win-rates on every matchup-stage combination.\n\nWhen do I play best?\n\nAverage win-rate on every day of the week.\nAverage win-rate at different times of day (e.g. Morning)\nAverage win-rate on every day of the week and at every time of day."
  },
  {
    "objectID": "projects/Melee-SQL.html#section-ii.-data-collection",
    "href": "projects/Melee-SQL.html#section-ii.-data-collection",
    "title": "Super Smash Brothers Player Statistics",
    "section": "",
    "text": "I used Slippi DB to parse replay files and place them into a SQLite database.\nI’ve been accumulating replays over the course of approximately a year (since I started playing in 2023).\n\nTotal number of replays: 3313"
  },
  {
    "objectID": "projects/Melee-SQL.html#section-iii.-sql-queries",
    "href": "projects/Melee-SQL.html#section-iii.-sql-queries",
    "title": "Super Smash Brothers Player Statistics",
    "section": "",
    "text": "-- Starting by filtering down to only the games where I play Falco\nWITH\nfalco_games AS (\n    select\n        me.character as bird \n    from players me, players op \n    where me.id != op.id -- x /= y (there are two unique objects in the domain)\n    and me.game_id = op.game_id -- Gxy & Gyx (One object is in the same game as the other, and vice versa)\n    and me.character='FALCO' -- Cx'Falco'  (One object has the property of playing Falco)\n    and me.code='FROG#671' -- Ox'FROG#671' (That object also has my specific code)\n) \nselect bird as Character, count(*) as \"Games played\" -- Counting Games\nFROM falco_games;\nOutput:\n\n\n\nCharacter\nGames played\n\n\n\n\nFALCO\n2332\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id, \n        me.game_id, \n        op.character as matchups, \n        me.winner as didIwin\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nselect matchups as 'Matchups',\n    round((sum(didIwin)*1.0 /count(*)*100), 1) as Winrate, \n    -- Winrate: The percetentage of time I win...\n    count(*) as 'Games played'\nFROM falco_games\nGROUP BY matchups                                           -- ... each matchup\nORDER BY Winrate desc;\nOutput, ordered by Winrate:\n\n\n\nMatchups\nWinrate\nGames played\n\n\n\n\nMEWTWO\n66.7\n9\n\n\nGAME_AND_WATCH\n66.7\n12\n\n\nBOWSER\n66.7\n3\n\n\nYOUNG_LINK\n63.6\n11\n\n\nROY\n60\n15\n\n\nPIKACHU\n57.1\n14\n\n\nSAMUS\n56.7\n30\n\n\nCAPTAIN_FALCON\n54.7\n338\n\n\nMARIO\n52.9\n34\n\n\nDONKEY_KONG\n52.9\n51\n\n\nSHEIK\n52.8\n159\n\n\nFOX\n51.3\n392\n\n\nFALCO\n48.7\n622\n\n\nYOSHI\n46.2\n13\n\n\nNESS\n45.5\n11\n\n\nLUIGI\n45.5\n44\n\n\nMARTH\n44.9\n365\n\n\nKIRBY\n42.9\n7\n\n\nZELDA\n40\n5\n\n\nGANONDORF\n34.2\n76\n\n\nLINK\n33.3\n21\n\n\nICE_CLIMBERS\n33.3\n6\n\n\nPEACH\n31.3\n48\n\n\nJIGGLYPUFF\n27.3\n33\n\n\nDR_MARIO\n25\n12\n\n\nPICHU\n0\n1\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id, \n        me.game_id as gid, \n        me.winner as winner\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nselect stage as Stage,\n   round((sum(winner)*1.0/count(*)*100),1) as Winrate,\n    count(*)\nFrom falco_games\nLEFT JOIN games on gid = games.id --Left joining with games to get stage data\nGROUP BY Stage\nHAVING count(*) &gt; 10 -- Ignore games on stages I've played less than 10 times\nORDER BY Winrate desc;\nOutput, ordered by Winrate:\n\n\n\nStage\nWinrate\ncount(*)\n\n\n\n\nFINAL_DESTINATION\n52.6\n382\n\n\nPOKEMON_STADIUM\n52.4\n393\n\n\nFOUNTAIN_OF_DREAMS\n47.7\n392\n\n\nYOSHIS_STORY\n46.8\n389\n\n\nDREAM_LAND_N64\n46.6\n371\n\n\nBATTLEFIELD\n46.1\n399\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id as mid, \n        me.game_id as gid,  \n        op.character as matchup, \n        me.winner as winner\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nSelect stage as Stage, matchup,\n    round((sum(winner)*1.0/count(*)*100), 1) as Winrate,\n    count(*)\nFrom falco_games\nLEFT JOIN games on gid = games.id\nGROUP BY Stage, matchup\nHAVING count(*) &gt;= 10 -- filtering out stage-matchup combinations that I have played on/against less than 10 times\nORDER BY Winrate desc;\nOutput, ordered by Winrate:\n\n\n\nStage\nmatchup\nWinrate\ncount(*)\n\n\n\n\nYOSHIS_STORY\nDONKEY_KONG\n70\n10\n\n\nFINAL_DESTINATION\nSHEIK\n62.5\n32\n\n\nBATTLEFIELD\nCAPTAIN_FALCON\n61.3\n62\n\n\nPOKEMON_STADIUM\nFOX\n61.3\n62\n\n\nFOUNTAIN_OF_DREAMS\nCAPTAIN_FALCON\n60.3\n58\n\n\nPOKEMON_STADIUM\nLUIGI\n60\n10\n\n\nBATTLEFIELD\nSHEIK\n57.1\n28\n\n\nFOUNTAIN_OF_DREAMS\nSHEIK\n57.1\n21\n\n\nFINAL_DESTINATION\nMARTH\n56.3\n64\n\n\nFINAL_DESTINATION\nFOX\n55.1\n69\n\n\nYOSHIS_STORY\nCAPTAIN_FALCON\n54.4\n57\n\n\nFINAL_DESTINATION\nFALCO\n53.8\n93\n\n\nDREAM_LAND_N64\nCAPTAIN_FALCON\n53.7\n54\n\n\nPOKEMON_STADIUM\nFALCO\n53.7\n108\n\n\nPOKEMON_STADIUM\nSHEIK\n51.9\n27\n\n\nFINAL_DESTINATION\nGANONDORF\n50\n14\n\n\nFOUNTAIN_OF_DREAMS\nMARTH\n50\n54\n\n\nPOKEMON_STADIUM\nCAPTAIN_FALCON\n50\n52\n\n\nYOSHIS_STORY\nFOX\n48.5\n66\n\n\nDREAM_LAND_N64\nFOX\n48.1\n54\n\n\nDREAM_LAND_N64\nFALCO\n47.4\n97\n\n\nFINAL_DESTINATION\nCAPTAIN_FALCON\n47.3\n55\n\n\nBATTLEFIELD\nFALCO\n47.1\n102\n\n\nFOUNTAIN_OF_DREAMS\nFOX\n47\n66\n\n\nBATTLEFIELD\nFOX\n46.6\n73\n\n\nYOSHIS_STORY\nFALCO\n45.7\n105\n\n\nBATTLEFIELD\nPEACH\n45.5\n11\n\n\nFOUNTAIN_OF_DREAMS\nFALCO\n45.3\n117\n\n\nDREAM_LAND_N64\nSHEIK\n44.8\n29\n\n\nDREAM_LAND_N64\nGANONDORF\n43.8\n16\n\n\nPOKEMON_STADIUM\nMARTH\n43.5\n69\n\n\nYOSHIS_STORY\nMARTH\n41.7\n60\n\n\nYOSHIS_STORY\nSHEIK\n40.9\n22\n\n\nDREAM_LAND_N64\nMARTH\n39.7\n58\n\n\nBATTLEFIELD\nMARTH\n39\n59\n\n\nFOUNTAIN_OF_DREAMS\nGANONDORF\n36.4\n11\n\n\nDREAM_LAND_N64\nDONKEY_KONG\n30\n10\n\n\nYOSHIS_STORY\nGANONDORF\n28.6\n14\n\n\nPOKEMON_STADIUM\nGANONDORF\n27.3\n11\n\n\nBATTLEFIELD\nGANONDORF\n10\n10\n\n\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id as mid, \n        me.game_id as gid, \n        me.winner as winner\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nselect\nCASE CAST (strftime('%w', DATE(start_time)) as int)   --strftime(%w,...) to get the day of the week based on the date of the game\n    WHEN 0 then 'Sunday'-- Instead of expressing each day of week as an integer (sunday=0... saturday=6), strings\n    WHEN 1 then 'Monday'\n    WHEN 2 then 'Tuesday'\n    WHEN 3 then 'Wednesday'\n    WHEN 4 then 'Thursday'\n    WHEN 5 then 'Friday'\n    else 'Saturday' end as \"Day of Week\",\n    round((sum(winner)*1.0/count(*)*100), 1) as Winrate,\n    count(*) as Games\nFrom falco_games\nLEFT JOIN games on gid = games.id\nGROUP BY 1\nORDER BY Winrate desc;\nOutput, ordered by Winrate:\n\n\n\nDay of Week\nWinrate\nGames\n\n\n\n\nWednesday\n52.3\n369\n\n\nThursday\n50.9\n379\n\n\nSaturday\n48.1\n318\n\n\nTuesday\n47.5\n396\n\n\nMonday\n47.4\n388\n\n\nFriday\n47.2\n299\n\n\nSunday\n46.4\n183\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id as mid, \n        me.game_id as gid, \n        me.winner as winner\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nselect \n    CASE -- Defining different times of the day based on the hour of the day\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 0 AND 5 THEN 'Late Night (12AM-5AM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 5 AND 9 THEN 'Morning (5AM-9AM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 9 AND 14 THEN 'Noon (9AM-2PM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 14 AND 19 THEN 'Afternoon (2PM-7PM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) &gt;= 19 THEN 'Evening (7PM-12AM)'\n    END AS \"Time of Day\",\n    round((sum(winner)*1.0/count(*)*100), 1) as Winrate,\n    count(*) as \"Games Played\"\nFrom falco_games\nLEFT JOIN games on gid = games.id\nGROUP BY 1\nORDER BY CASE -- Aesthetic reordering so 'Morning' comes first, and 'Late Night' comes last\n    WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 0 AND 5 THEN 5\n    WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 5 AND 9 THEN 1\n    WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 9 AND 14 THEN 2\n    WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 14 AND 19 THEN 3\n    WHEN CAST (strftime('%H', TIME(start_time)) as int) &gt;= 19 THEN 4\nEND;\nOutput, ordered by Winrate:\n\n\n\nTime of Day\nWinrate\nGames Played\n\n\n\n\nMorning (5AM-9AM)\n47.8\n370\n\n\nNoon (9AM-2PM)\n61.5\n13\n\n\nAfternoon (2PM-7PM)\n48.6\n210\n\n\nEvening (7PM-12AM)\n46.2\n650\n\n\nLate Night (12AM-5AM)\n50.5\n1089\n\n\n\n\n\n\nWITH\nfalco_games AS (\n    select me.id as mid, \n        me.game_id as gid, \n        me.winner as winner\n    from players me, players op \n    where me.id != op.id\n    and me.game_id = op.game_id\n    and me.character='FALCO'\n    and me.code='FROG#671'\n)\nselect\n    CASE CAST (strftime('%w', DATE(start_time)) as int) \n        WHEN 0 then 'Sunday'\n        WHEN 1 then 'Monday'\n        WHEN 2  then 'Tuesday'\n        WHEN 3 then 'Wednesday'\n        WHEN 4 then 'Thursday'\n        WHEN 5 then 'Friday'\n        else 'Saturday' \n    END as 'Day of Week',\n    CASE \n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 0 AND 5 THEN 'Late Night (12AM-5AM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 5 AND 9 THEN 'Morning (5AM-9AM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 9 AND 14 THEN 'Noon (9AM-2PM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) BETWEEN 14 AND 19 THEN 'Afternoon (2PM-7PM)'\n        WHEN CAST (strftime('%H', TIME(start_time)) as int) &gt;= 19 THEN 'Evening (7PM-12AM)'\n    END AS 'Time of Day',\n    round((sum(winner)*1.0/count(*)*100), 1) as Winrate,\n    count(*) as Games\nFrom falco_games\nLEFT JOIN games on gid = games.id\nGROUP BY 1, 2\nHAVING Games &gt;= 10\nORDER BY Winrate desc;\nOutput, ordered by Winrate:\n\n\n\nDay of Week\nTime of Day\nWinrate\nGames\n\n\n\n\nThursday\nLate Night (12AM-5AM)\n56\n184\n\n\nWednesday\nAfternoon (2PM-7PM)\n56\n75\n\n\nWednesday\nEvening (7PM-12AM)\n55.1\n78\n\n\nMonday\nMorning (5AM-9AM)\n54.5\n33\n\n\nSunday\nAfternoon (2PM-7PM)\n54.5\n11\n\n\nTuesday\nMorning (5AM-9AM)\n53.3\n15\n\n\nTuesday\nAfternoon (2PM-7PM)\n53.1\n32\n\n\nSaturday\nLate Night (12AM-5AM)\n52.4\n187\n\n\nWednesday\nLate Night (12AM-5AM)\n52.4\n126\n\n\nMonday\nLate Night (12AM-5AM)\n50\n184\n\n\nSaturday\nMorning (5AM-9AM)\n50\n32\n\n\nWednesday\nNoon (9AM-2PM)\n50\n10\n\n\nSunday\nMorning (5AM-9AM)\n49.4\n89\n\n\nFriday\nMorning (5AM-9AM)\n48.8\n43\n\n\nFriday\nAfternoon (2PM-7PM)\n47.5\n40\n\n\nTuesday\nLate Night (12AM-5AM)\n47.1\n261\n\n\nFriday\nLate Night (12AM-5AM)\n46.9\n98\n\n\nFriday\nEvening (7PM-12AM)\n46.6\n118\n\n\nWednesday\nMorning (5AM-9AM)\n46.3\n80\n\n\nThursday\nEvening (7PM-12AM)\n46.2\n106\n\n\nTuesday\nEvening (7PM-12AM)\n45.5\n88\n\n\nSunday\nLate Night (12AM-5AM)\n44.9\n49\n\n\nSaturday\nEvening (7PM-12AM)\n44.7\n76\n\n\nMonday\nEvening (7PM-12AM)\n44\n150\n\n\nThursday\nMorning (5AM-9AM)\n42.3\n78\n\n\nSunday\nEvening (7PM-12AM)\n38.2\n34\n\n\nMonday\nAfternoon (2PM-7PM)\n38.1\n21\n\n\nSaturday\nAfternoon (2PM-7PM)\n21.7\n23"
  },
  {
    "objectID": "projects/Melee-SQL.html#section-iv.-interpretation-and-key-insights",
    "href": "projects/Melee-SQL.html#section-iv.-interpretation-and-key-insights",
    "title": "Super Smash Brothers Player Statistics",
    "section": "",
    "text": "I need practice against Jigglypuff.\nI need practice on Battlefield.\nCounterpick to Final Destination against Sheik, Marth and Fox.\nAvoid Battlefield against Marth, Peach, Fox and Falco.\nAgainst Captain Falcon, counterpick to Battlefield and avoid Final Destination. ### Matchups First, looking at my matchup win-rates, we can see that my best matchups are against Mewtwo, Bowser, and Game and Watch. This makes sense because these characters are generally considered weak characters according to the community tier-list (i.e., a “low-tier” character), which also explains why I’ve played against so few of them. Compare this to someone like Captain Falcon, who I’ve played against many more times (n=338). My matchups against high-tiers generally matter more than against non-high-tiers, simply because more people play them.\n\n\nMy highest win-rate against a “high-tier” character is 54.7% against Captain Falcon, and my lowest is 27.3% against Jigglypuff. This suggests that I need more practice against Jigglypuff, and that I’m well practiced against Captain Falcon.\n\n\n\nLooking at my stage win-rates, it’s interesting to see that Final Destination is my best stage. Traditional wisdom suggests Falco, my character, is bad on Final Destination. This is bizarre, but situationally advantageous when counter-picking a stage in tournament. 1 Looking at my stage-matchup win-rates, I am more than 50% likely to beat Sheik, Marth and Fox on Final Destination, who all generally like to counterpick to Final Destination. However, my win-rate against Captain Falcon is losing on Final Destination at 47.3%. So, if I beat Sheik, Marth or Fox in game 1, they might unknowingly counterpick my best stage. If I lose in game 1, I know to counterpick to Final Destination, unless I’m playing against Captain Falcon.\nFurthermore, Battlefield is my worst stage, which according to traditional wisdom should be one of Falco’s best stages. Looking at my stage-matchup win-rates, I can see that I am more likely to lose on Battlefield against Marth, Peach, Fox, and Falco. So, I should avoid Battlefield against Marth, Peach, Fox and Falco whenever possible. But funnily enough, Captain Falcon is again the exception. My win-rate against Captain Falcon on Battlefield is my third best mathcup at 61.3%. So, I should pick Battlefield against Captain Falcon.\n\n\n\nI was very surprised to see that I play the best on Wednesdays. This was also reflected in the Day of the Week - Time of Day win-rates, where my top 3 performances were on varying times on Wednesday. 2 I can only speculate, but I suspect this outcome is a result of my and other people’s work schedules. For around a year Wednesdays were my day off, which gave me more time to relax and play Melee. Generally, people don’t have Wednesdays off, so my opponents were probably suffering from the stress of work on Wednesdays. I believe it was a combination of my atypical and other people’s typical work schedule. This simply suggests that stress makes people perform worse in Melee."
  },
  {
    "objectID": "projects/Melee-SQL.html#footnotes",
    "href": "projects/Melee-SQL.html#footnotes",
    "title": "Super Smash Brothers Player Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTournament sets are usually best out of 3. Rock, Paper, Scissors determines who picks the stage for the first game. The loser of the first game gets to pick the stage for the second game. The strategy of picking a stage that favors your character over the opponent’s is called counter-picking.↩︎\nThursday at 12-5AM is my highest winrate day-hour, which is technically a calendar-Thursday. However, it’s better to interpret it as a Wednesday play session.↩︎"
  }
]