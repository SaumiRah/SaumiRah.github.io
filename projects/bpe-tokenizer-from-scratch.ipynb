{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2536,"sourceType":"datasetVersion","datasetId":1366}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Why Tokenize?","metadata":{}},{"cell_type":"markdown","source":"Let's motivate the need for a tokenizer:\n\n> \"What is AI and how does it work?\"\n\nAI Chatbots are large language models (LLMs). LLMs are language models: they are machine learning models that predict the next word given a sequence of words, and they are large: *a lot* of parameters (GPT4 had 1.76 trillion). LLMs are **trained** (i.e. it's parameters are tuned) on huge amounts of text (often from the internet); the end result is a computer program that represents natural language really well.\n\n> \"How do we train an LLM?\"\n\nTo train an LLM, we give the model a sentence from our corpus (i.e. all our training data we scraped from the internet). Starting from the first word, have it predict the next word in the sentence. We calculate *how wrong* it's prediction was, based on the difference in **meaning** between the word it predicted and the actual next word in the sentence. The model learns from it's mistake, i.e. its parameters are tweaked such that the next time it's in a similar situation it'll make a better prediction.\n\n> \"How does an LLM compare the meaning of words?\"\n\nIt's not obvious how a computer could quantifiably represent the meaning of a word, or how it could calculate the difference in meaning between two words. How could numbers tell us \"dog\" is closer to \"puppy\" and farther than \"god\" in meaning? The answer involves a bit of philosophy. The Distributional Hypothesis states a word's meaning is determined by the contexts its used in, i.e. the other words around it. We know \"dog\" is close to \"puppy\" because they are *used* in similar situations. Here's one simple way of determining a word's context: We can determine what \"dog\" means by looking for the word in our corpus; whenever we see it, we look to it's left and right and note which words surround it, keeping a tally of the number of times a given word is in the context of \"dog\". Thus, we get a list of numbers representing the context \"dog\" is used in (this list of numbers is called a word embedding).\n\n> \"Okay, but what counts as a word?\"\n\nVery good question! In order to get individual words to train an LLM with, we need to split up raw strings of text into discrete words. In an English sentence each word is generally separated by spaces. But, we often care about sub-word units of text. For example, we might want to learn what the suffix \"ology\" (as in \"Biology\", \"Ontology\") means in itself. These sub-word units are called **tokens**; they can be sequences of characters like with \"ology\", or as granular as individual characters (though individual characters would not a good choice of token for most NLP tasks).\n\nTurning a raw string into a list of tokens is called **Tokenization**, and it's the first step of training any machine learning model on text. Notice that tokenization requires much more thought than simply splitting words on spaces. Tokenization involves creating a token vocabulary out of our raw text, and splitting up our text string using this vocabulary.","metadata":{}},{"cell_type":"markdown","source":"# Byte-Pair Encoding\nAKA BPE\n\n- Pre-tokenize your sentence(s)\n    - could be as simple as splitting on spaces, or a more sophisticated rules based approach\n- Count frequencies of each pre-token\n    - e.g. assume after pre-tokenization we get the following words and frequencies `(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)`\n- Split each word by their characters and compile a set of all unique symbols (symbol vocabulary)\n    - `(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)`\n    - symbol vocabulary: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`\n\n- Then count the frequencies of each possible vocabulary pair and pick the one that occurs most frequent. The most frequent pair of symbols is added to the symbol vocabulary.\n    - `ug` is the most frequent pair. This is the first merge rule.\n    - `(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)`\n    - symbol vocabulary: `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]`\n- Continue until you have a desire symbol vocabulary size (hyperparameter)\n\nBecause there are a lot of base characters in unicode, our base vocabulary might be very large, which would make iterating through pairs more difficult computationally. We can save on computation by using bytes (e.g. utf-8) as the base vocabulary.","metadata":{}},{"cell_type":"markdown","source":"## Importing Data","metadata":{}},{"cell_type":"code","source":"# importing dependencies\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:38:37.401348Z","iopub.execute_input":"2026-02-09T23:38:37.401772Z","iopub.status.idle":"2026-02-09T23:38:37.406903Z","shell.execute_reply.started":"2026-02-09T23:38:37.401735Z","shell.execute_reply":"2026-02-09T23:38:37.405633Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"# importing data: using bible corpus\ntext = pd.read_csv('/kaggle/input/bible/t_asv.csv', usecols=['t'])\n\n# splitting train and test\ntrain = text.iloc[:round(len(text)*0.8)]\ntest = text.iloc[round(len(text)*0.8):]\nprint(\"Train set size:\",len(train))\nprint(\"Test set size:\", len(test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T22:23:31.287086Z","iopub.execute_input":"2026-02-09T22:23:31.287540Z","iopub.status.idle":"2026-02-09T22:23:31.417465Z","shell.execute_reply.started":"2026-02-09T22:23:31.287510Z","shell.execute_reply":"2026-02-09T22:23:31.416247Z"}},"outputs":[{"name":"stdout","text":"Train set size: 24882\nTest set size: 6221\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## BPE Tokenizer Class","metadata":{}},{"cell_type":"code","source":"class BPETokenizer():\n    def __init__(self):\n        self.token2idx = dict()\n        self.idx2token = dict()\n        self.pretoken_freq = Counter()\n        self.token_freq = Counter()\n        self.pair_freq = None\n        self.pretoken2token = dict()\n    # pre_tokenize\n        # takes text\n        # inits frequencies of pre_tokenized words, token2idx and idx2token\n    # training function\n        # takes text and vocab size\n        # iteratively creates token pairs based on pair frequency\n    # tokenizer function\n        # takes text\n        # calls pre-tokenizer \n        # returns tokenized text as list\n    \n    def pre_tokenize(self, text):\n        \"\"\"\n        Takes a raw string of text and performs pre-tokenization.\n        Lowercases, splits on spaces and puntuation.\n        \"\"\"\n        vocab = set()\n        pre_tokenized_sentence = list()\n        #iterate through sentences: clean, split \n        for sent in text:\n            sent_str = sent.lower().replace('.', '') #\n            s = re.split(r'[;:,\\s]+', sent_str)\n            for chars in s:\n                pre_tokenized_sentence.append(tuple(chars))\n                self.pretoken_freq.update([tuple(chars)])\n                vocab.update(chars)\n\n        #creating token index\n        for idx, char in enumerate(vocab):\n            self.token2idx[char] = idx\n\n        #creating reverse index\n        self.idx2token = {val:key for key, val in self.token2idx.items()}\n        \n        return pre_tokenized_sentence\n    \n    def train(self, text, num_tokens):\n        \"\"\"\n        Takes raw text, pre-tokenizes it, and then performs BPE encoding until vocab size is default characters + num_tokens.\n        \"\"\"\n        #init token_freq dict and token2idx dict\n        self.pre_tokenize(text)\n        \n        self.token_freq = self.pretoken_freq\n        \n        \n        for _ in range(num_tokens):\n            # numpy array of size len(token2idx) * len(token2idx), populate cells with tokens from token_freq\n            self.pair_freq = np.zeros((len(self.token2idx.keys()), len(self.token2idx.keys())))\n            \n            # populating \n            for key, value in self.token_freq.items():\n                key_idx = [self.token2idx.get(k) for k in key]\n                for i in range(1, len(key_idx)):\n                    self.pair_freq[key_idx[i-1], key_idx[i]] += value #populating cells token_pair cells with pair frequencies\n                \n            # row and col indices of maximum cell: represents most frequent sequence of tokens, will be combined into one new token\n            row, col = np.unravel_index(self.pair_freq.argmax(), self.pair_freq.shape)\n            \n            # combine the most common token pairs into a new token\n            tok_1 = self.idx2token[row]\n            tok_2 = self.idx2token[col]\n            new_token = tok_1 + tok_2\n\n            # adding new_token to token2idx and idx2token\n            self.token2idx[new_token]= max(self.token2idx.values()) + 1\n            self.idx2token[max(self.idx2token)+1] = new_token\n            \n            # need to make a copy of the dict because we can't change keys while we iterate through the dict\n            token_freq_copy = Counter()\n\n            #find all keys in token_freq containing the following sequence of components: tok_1, tok_2\n            #delete both, and replace with new_token where tok_1 was\n            for tup, val in self.token_freq.items():\n                new_tup = []\n                i = 0\n                while i < len(tup):\n                    # If we find a match for the pair, merge them\n                    if i < len(tup) - 1 and tup[i] == tok_1 and tup[i+1] == tok_2:\n                        new_tup.append(new_token)\n                        i += 2 # Skip both components\n                    else:\n                        new_tup.append(tup[i])\n                        i += 1\n                token_freq_copy[tuple(new_tup)] = val\n\n            self.token_freq = token_freq_copy\n\n        # creating pretoken2token dict\n        self.pretoken2token = {list(self.pretoken_freq.keys())[i] : list(self.token_freq.keys())[i] for i in range(len(self.pretoken_freq))}\n        \n        return self.token_freq #optional return\n    \n    def tokenize(self, sent):\n        \"\"\"\n        Tokenizes sentence based on a token vocabulary learned during training.\n        Must run `train` method first.\n        \n        Parameters\n        ----------\n        sentence : the str to be tokenized\n\n        Returns\n        -------\n        list : list of tokenized sentence(s)\n        \n        \"\"\"\n        #handling error: tokenizing without training\n        if not self.pretoken2token:\n            raise AttributeError(\"Must train tokenizer first. Run `train` on training data.\")\n        #handling multiple input types: if a single string is implemented, wrap it in a list\n        if isinstance(sent, str):\n            sent = [sent]\n\n        # returning list of tuples of tokens\n        # if word isn't in our tokenizer vocab, just return the untokenized word as is\n        return [self.pretoken2token.get(tup, tuple([''.join(tup)])) for tup in self.pre_tokenize(sent)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:27:19.639523Z","iopub.execute_input":"2026-02-09T23:27:19.639868Z","iopub.status.idle":"2026-02-09T23:27:19.657538Z","shell.execute_reply.started":"2026-02-09T23:27:19.639828Z","shell.execute_reply":"2026-02-09T23:27:19.656673Z"}},"outputs":[],"execution_count":101},{"cell_type":"markdown","source":"## Tests","metadata":{}},{"cell_type":"code","source":"# tests\ntokenizer = BPETokenizer()\ntest_sentence = test.iloc[0,0]\nprint(\"Test sentence:\\n\", test_sentence, \"\\n\")\nfreqs = tokenizer.train(train.iloc[:, 0], 3000) #training on entire training set, adding 3000 vocab tokens\ntokens = tokenizer.tokenize(test_sentence)\nprint(\"Tokenized test sentence:\\n\", tokens)\nassert len(tokens) == 27\nassert all(isinstance(i, tuple) for i in tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:27:21.442956Z","iopub.execute_input":"2026-02-09T23:27:21.443361Z","iopub.status.idle":"2026-02-09T23:30:35.543484Z","shell.execute_reply.started":"2026-02-09T23:27:21.443330Z","shell.execute_reply":"2026-02-09T23:30:35.542552Z"}},"outputs":[{"name":"stdout","text":"Test sentence:\n Now when he was risen early on the first day of the week, he appeared first to Mary Magdalene, from whom he had cast out seven demons. \n\nTokenized test sentence:\n [('now',), ('when',), ('he',), ('was',), ('ris', 'en'), ('early',), ('on',), ('the',), ('first',), ('day',), ('of',), ('the',), ('we', 'ek'), ('he',), ('appeared',), ('first',), ('to',), ('mar', 'y'), ('mag', 'd', 'al', 'ene'), ('from',), ('whom',), ('he',), ('had',), ('cast',), ('out',), ('seven',), ('demon', 's')]\n","output_type":"stream"}],"execution_count":102}]}